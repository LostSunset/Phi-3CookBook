
| 1x | ~42 ajuste fino completo | 1 | 8 | ✔ | 64 | 8.657 | 1.72x | ~36 ajuste fino completo | 2 | 16 | ✔ | 64 | 16.903 | 3.35x | ~29 ajuste fino completo | 4 | 32 | ✔ | 64 | 33.433 | 6.63x | ~26 modelo de imagen congelada | 1 | 8 | | 64 | 17.578 | 3.49x | ~29 modelo de imagen congelada | 1 | 8 | ✔ | 64 | 31.736 | 6.30x | ~27 LoRA | 1 | 8 | | 64 | 5.591 | 1.11x | ~50 LoRA | 1 | 8 | ✔ | 64 | 12.127 | 2.41x | ~16 QLoRA | 1 | 8 | | 64 | 4.831 | 0.96x | ~32 QLoRA | 1 | 8 | ✔ | 64 | 10.545 | 2.09x | ~10 ### 8x V100-32GB (Volta) Método de entrenamiento | \# nodos | GPUs | flash attention | Tamaño de lote efectivo | Rendimiento (img/s) | Aceleración | Memoria máxima GPU (GB) --- | --- | --- | --- | --- | --- | --- | --- | ajuste fino completo | 1 | 8 | | 64 | 2.462 | 1x | ~32 ajuste fino completo | 2 | 16 | | 64 | 4.182 | 1.70x | ~32 ajuste fino completo | 4 | 32 | | 64 | 5.465 | 2.22x | ~32 modelo de imagen congelada | 1 | 8 | | 64 | 8.942 | 3.63x | ~27 LoRA | 1 | 8 | | 64 | 2.807 | 1.14x | ~30 ## Problemas conocidos - No se puede ejecutar flash attention con fp16 (siempre se recomienda bf16 cuando esté disponible, y todas las GPUs que admiten flash attention también admiten bf16). - No se admite guardar puntos de control intermedios y reanudar el entrenamiento aún.

Aviso legal: La traducción fue realizada a partir del original por un modelo de IA y puede no ser perfecta. 
Por favor, revise el resultado y haga las correcciones necesarias.