# **使用 llama.cpp 量化 Phi-3.5**

## **什么是 llama.cpp**

llama.cpp 是一个主要用 C++ 编写的开源软件库，用于对各种大型语言模型（LLMs）进行推理，例如 Llama。其主要目标是为 LLM 推理提供最先进的性能，适用于各种硬件，且设置简单。此外，该库还提供了 Python 绑定，提供了用于文本补全的高级 API 和兼容 OpenAI 的 Web 服务器。

llama.cpp 的主要目标是实现 LLM 推理的最小化设置和最先进的性能，适用于各种硬件——无论是本地还是云端。

- 纯 C/C++ 实现，无任何依赖
- 苹果芯片是头等公民——通过 ARM NEON、Accelerate 和 Metal 框架进行优化
- 支持 x86 架构的 AVX、AVX2 和 AVX512
- 1.5 位、2 位、3 位、4 位、5 位、6 位和 8 位整数量化，以实现更快的推理和减少内存使用
- 为在 NVIDIA GPU 上运行 LLM 而定制的 CUDA 内核（通过 HIP 支持 AMD GPU）
- 支持 Vulkan 和 SYCL 后端
- CPU+GPU 混合推理，以部分加速超过总 VRAM 容量的模型

## **使用 llama.cpp 量化 Phi-3.5**

可以使用 llama.cpp 对 Phi-3.5-Instruct 模型进行量化，但目前不支持 Phi-3.5-Vision 和 Phi-3.5-MoE。llama.cpp 转换的格式为 gguf，这也是最广泛使用的量化格式。

在 Hugging face 上有大量的量化 GGUF 格式模型。AI Studio、Ollama 和 LlamaEdge 依赖于 llama.cpp，因此 GGUF 模型也经常被使用。

### **什么是 GGUF**

GGUF 是一种二进制格式，优化用于快速加载和保存模型，使其在推理方面非常高效。GGUF 设计用于与 GGML 和其他执行器配合使用。GGUF 由 @ggerganov 开发，他也是流行的 C/C++ LLM 推理框架 llama.cpp 的开发者。最初在 PyTorch 等框架中开发的模型可以转换为 GGUF 格式，以便与这些引擎一起使用。

### **ONNX 与 GGUF 的比较**

ONNX 是一种传统的机器学习/深度学习格式，在不同的 AI 框架中得到了很好的支持，并且在边缘设备中有很好的使用场景。而 GGUF 基于 llama.cpp，可以说是在 GenAI 时代诞生的。两者的用途相似。如果你希望在嵌入式硬件和应用层中获得更好的性能，ONNX 可能是你的选择。如果你使用 llama.cpp 的衍生框架和技术，那么 GGUF 可能会更好。

### **使用 llama.cpp 量化 Phi-3.5-Instruct**

**1. 环境配置**

```bash
git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8
```

**2. 量化**

使用 llama.cpp 将 Phi-3.5-Instruct 转换为 FP16 GGUF

```bash
./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf
```

将 Phi-3.5 量化为 INT4

```bash
./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M
```

**3. 测试**

安装 llama-cpp-python

```bash
pip install llama-cpp-python -U
```

***Note***

如果你使用苹果芯片，请按以下方式安装 llama-cpp-python

```bash
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U
```

测试

```bash
llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10
```

## **资源**

1. 了解更多关于 llama.cpp 的信息 [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. 了解更多关于 GGUF 的信息 [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

免责声明：此翻译由AI模型从原文翻译而来，可能并不完美。请审核输出内容并进行必要的修改。