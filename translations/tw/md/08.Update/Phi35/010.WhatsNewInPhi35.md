# **Phi-3.5 家族的新功能**

你已经在使用 Phi-3 家族了吗？你的使用场景是什么？在 2024 年 8 月 20 日，微软发布了新的 Phi-3.5 家族，该版本在多语言、视觉和 AI 代理方面得到了增强。让我们结合 Hugging face 上的 Model Card 进行更详细的介绍。

![PhiFamily](../../../../../translated_images/Phi3getstarted.086dfb90bb69325da6b717586337f2aec5decc241fda85e322eb55c709167f73.tw.png)


## **Phi-3.5-mini-instruct**

Phi-3.5-mini 是一个轻量级的、最先进的开源模型，基于用于 Phi-3 的数据集构建——合成数据和过滤的公开网站数据——并专注于高质量、推理密集的数据。该模型属于 Phi-3 模型家族，支持 128K 令牌上下文长度。模型经过严格的增强过程，结合了监督微调、近端策略优化和直接偏好优化，以确保精确的指令遵循和强大的安全措施。

![benchmark1](../../../../../translated_images/benchmark1.479cb048e7d9239b09e562c410a54f6c9eaf85030af67ac6e7de80a69e4778a5.tw.png)

![benchmark2](../../../../../translated_images/benchmark2.76982d411a07caa3ebd706dd6c0ba98b98a5609de371176a67cd619d70d4e6da.tw.png)

通过基准测试上的指标，可以看到 Phi-3.5-mini 相比于 Phi-3-mini，在多语言和长文本内容支持方面有所提升，这些提升用于增强 Phi-3.5 mini 在边缘应用中的语言和文本能力。

我们可以通过 GitHub Models 比较中文知识的能力。当我们问“长沙在哪里？”时，可以比较 Phi-3-mini-128k-instruct 和 Phi-3.5-mini-128k-instruct 的结果。

![Phi3](../../../../../translated_images/gh3.6b1a5c38ed732e40c0effaf4c558badfab0be6148b194aa6bec44adbfb1e4342.tw.png)

![Phi35](../../../../../translated_images/gh35.b0fd2ff379a5f2d995ea1faedd2d7260cfcad7ffbad5a721a8a1b2b3d84028c8.tw.png)

不难看出，在中文语料库上的数据增强使得 Phi-3.5-mini 在基础文本生成场景中有更好的结果（***注意：*** 请注意，如果 Phi-3.5-mini 需要更准确的答案，建议根据应用场景进行微调）

## **Phi-3.5-vision-instruct**

Phi-3.5-vision 是一个轻量级的、最先进的开源多模态模型，基于包括合成数据和过滤的公开网站数据的数据集构建，专注于高质量、推理密集的文本和视觉数据。该模型属于 Phi-3 模型家族，多模态版本支持 128K 令牌上下文长度。模型经过严格的增强过程，结合了监督微调和直接偏好优化，以确保精确的指令遵循和强大的安全措施。

通过 Vision 我们让 Phi-3.x 家族开了眼界，并能够完成以下场景

1. 内存/计算受限环境
2. 延迟敏感场景
3. 一般图像理解
4. 光学字符识别
5. 图表和表格理解
6. 多图像比较
7. 多图像或视频剪辑摘要

通过 Vision，我们让 Phi 家族开了眼界并完成以下场景

我们还可以使用提供的 Hugging face 基准测试来了解在不同视觉场景中的比较

![benchmark3](../../../../../translated_images/benchmark3.4d9484cc062f0c5076783f3cb33fe533c03995d3a5debc437420e88960032672.tw.png)

如果你想尝试 Phi-3.5-vision-instruct 的免费试用，我们可以使用 [Nivida NIM](https://build.nvidia.com/microsoft/phi-3_5-vision-instruct) 来完成体验。

![nim](../../../../../translated_images/nim.c985945596d6b2629658087485d16028a3874dcc37329de51b94adf09d0af661.tw.png)

当然，你也可以通过 Azure AI Foundry 完成部署。

## **Phi-3.5-MoE-instruct**

Phi-3.5-MoE 是一个轻量级的、最先进的开源模型，基于用于 Phi-3 的数据集构建——合成数据和过滤的公开文档——并专注于高质量、推理密集的数据。该模型支持多语言，支持 128K 令牌上下文长度。模型经过严格的增强过程，结合了监督微调、近端策略优化和直接偏好优化，以确保精确的指令遵循和强大的安全措施。

随着 AI 代理的发展，对 MoE 模型的需求将逐渐增加。MoE，全称为混合专家模型，是由多个专家模型混合形成的新模型。MoE 的思路是先将大问题拆分，再逐一解决小问题，然后总结结论。其次，模型规模是提升模型性能的关键因素之一。在计算资源有限的情况下，使用较少的训练步骤训练一个更大的模型往往比使用更多步骤训练一个较小的模型效果更好。

Phi-3.5-MoE-Instruct 模型比 Phi-3.5-Vision 和 Phi-3.5-Instruct 需要更多的计算能力。建议使用基于云的方法，如 Azure AI Foundry 和 Nvidia NIM 进行体验和使用。

![nim2](../../../../../translated_images/nim2.ab50cc468e987efe5e87e8b9b2927f751b6d080c4a146129c2133da94b0f781e.tw.png)

### **🤖 Phi-3.5 与 Apple MLX 的示例**

| 实验室    | 介绍 | 进入 |
| -------- | ------- |  ------- |
| 🚀 实验室-介绍 Phi-3.5 Instruct  | 学习如何使用 Phi-3.5 Instruct |  [进入](../../../../../code/09.UpdateSamples/Aug/phi3-instruct-demo.ipynb)    |
| 🚀 实验室-介绍 Phi-3.5 Vision（图像） | 学习如何使用 Phi-3.5 Vision 分析图像 |  [进入](../../../../../code/09.UpdateSamples/Aug/phi3-vision-demo.ipynb)    |
| 🚀 实验室-介绍 Phi-3.5 MoE   | 学习如何使用 Phi-3.5 Vision 分析图像 |  [进入](../../../../../code/09.UpdateSamples/Aug/phi3_moe_demo.ipynb)    |

## **资源**

1. Hugging face 的 Phi Family [https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)

2. 关于 GitHub Models [https://gh.io/models](https://gh.io/models)

3. 关于 Azure AI Foundry [https://ai.azure.com/](https://ai.azure.com/)

4. 关于 Nividia NIM [https://build.nvidia.com/explore/discover](https://build.nvidia.com/explore/discover)

**免責聲明**：
本文檔是使用機器翻譯服務翻譯的。我們努力確保準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。應以原語言的原始文件為權威來源。對於關鍵信息，建議使用專業人工翻譯。我們對因使用此翻譯而產生的任何誤解或誤讀不承擔責任。