# **Phi-3.5 패밀리 양자화**

모델 양자화는 신경망 모델의 파라미터(예: 가중치 및 활성화 값)를 큰 값 범위(일반적으로 연속적인 값 범위)에서 더 작은 유한 값 범위로 매핑하는 과정을 말합니다. 이 기술은 모델의 크기와 계산 복잡도를 줄이고, 모바일 장치나 임베디드 시스템과 같은 자원 제한 환경에서 모델의 운영 효율성을 향상시킬 수 있습니다. 모델 양자화는 파라미터의 정밀도를 줄여 압축을 달성하지만, 일정한 정밀도 손실도 가져옵니다. 따라서 양자화 과정에서는 모델 크기, 계산 복잡도 및 정밀도 사이의 균형을 맞추는 것이 필요합니다. 일반적인 양자화 방법으로는 고정 소수점 양자화, 부동 소수점 양자화 등이 있습니다. 특정 시나리오와 필요에 따라 적절한 양자화 전략을 선택할 수 있습니다.

우리는 GenAI 모델을 엣지 장치에 배포하고, 모바일 장치, AI PC/Copilot+PC, 전통적인 IoT 장치와 같은 GenAI 시나리오에 더 많은 장치가 참여할 수 있도록 하고자 합니다. 양자화 모델을 통해, 다양한 장치 기반으로 이를 다양한 엣지 장치에 배포할 수 있습니다. 하드웨어 제조업체가 제공하는 모델 가속 프레임워크와 양자화 모델을 결합하여, 더 나은 SLM 애플리케이션 시나리오를 구축할 수 있습니다.

양자화 시나리오에서는 다양한 정밀도(INT4, INT8, FP16, FP32)가 있습니다. 다음은 일반적으로 사용되는 양자화 정밀도에 대한 설명입니다.

### **INT4**

INT4 양자화는 모델의 가중치와 활성화 값을 4비트 정수로 양자화하는 급진적인 양자화 방법입니다. INT4 양자화는 표현 범위가 작고 정밀도가 낮아 더 큰 정밀도 손실을 초래할 수 있습니다. 그러나 INT8 양자화에 비해 INT4 양자화는 모델의 저장 요구 사항과 계산 복잡도를 더욱 줄일 수 있습니다. INT4 양자화는 실용적인 응용에서 비교적 드물게 사용되는데, 이는 너무 낮은 정밀도가 모델 성능에 심각한 저하를 초래할 수 있기 때문입니다. 또한 모든 하드웨어가 INT4 연산을 지원하지 않기 때문에, 양자화 방법을 선택할 때 하드웨어 호환성을 고려해야 합니다.

### **INT8**

INT8 양자화는 모델의 가중치와 활성화를 부동 소수점 숫자에서 8비트 정수로 변환하는 과정입니다. INT8 정수가 표현하는 숫자 범위는 작고 정밀도가 낮지만, 저장 및 계산 요구 사항을 크게 줄일 수 있습니다. INT8 양자화에서는 모델의 가중치와 활성화 값이 스케일링 및 오프셋을 포함한 양자화 과정을 거쳐, 원래의 부동 소수점 정보를 최대한 보존합니다. 추론 중에는 이러한 양자화된 값이 계산을 위해 다시 부동 소수점 숫자로 디양자화되며, 다음 단계에서는 다시 INT8로 양자화됩니다. 이 방법은 대부분의 응용에서 충분한 정밀도를 제공하면서 높은 계산 효율을 유지할 수 있습니다.

### **FP16**

FP16 형식, 즉 16비트 부동 소수점 숫자(float16)는 32비트 부동 소수점 숫자(float32)에 비해 메모리 사용량을 절반으로 줄이며, 대규모 딥러닝 응용에서 상당한 이점을 제공합니다. FP16 형식은 동일한 GPU 메모리 제한 내에서 더 큰 모델을 로드하거나 더 많은 데이터를 처리할 수 있게 합니다. 현대 GPU 하드웨어가 계속해서 FP16 연산을 지원함에 따라, FP16 형식을 사용하면 계산 속도 향상도 기대할 수 있습니다. 그러나 FP16 형식은 낮은 정밀도로 인해 일부 경우에 수치 불안정성이나 정밀도 손실을 초래할 수 있는 고유한 단점도 가지고 있습니다.

### **FP32**

FP32 형식은 더 높은 정밀도를 제공하며, 넓은 범위의 값을 정확하게 표현할 수 있습니다. 복잡한 수학적 연산이 수행되거나 고정밀 결과가 필요한 시나리오에서는 FP32 형식이 선호됩니다. 그러나 높은 정밀도는 더 많은 메모리 사용과 더 긴 계산 시간을 의미하기도 합니다. 대규모 딥러닝 모델의 경우, 특히 모델 파라미터가 많고 데이터 양이 방대한 경우, FP32 형식은 GPU 메모리 부족이나 추론 속도 저하를 초래할 수 있습니다.

모바일 장치나 IoT 장치에서는 Phi-3.x 모델을 INT4로 변환할 수 있으며, AI PC / Copilot PC에서는 INT8, FP16, FP32와 같은 더 높은 정밀도를 사용할 수 있습니다.

현재, 다양한 하드웨어 제조업체는 Intel의 OpenVINO, Qualcomm의 QNN, Apple's MLX, Nvidia의 CUDA 등과 같은 프레임워크를 통해 생성 모델을 지원하며, 모델 양자화를 결합하여 로컬 배포를 완료할 수 있습니다.

기술적으로, 양자화 후에는 PyTorch / Tensorflow 형식, GGUF, ONNX와 같은 다양한 형식 지원이 가능합니다. GGUF와 ONNX 간의 형식 비교 및 응용 시나리오를 수행했습니다. 여기서는 모델 프레임워크부터 하드웨어까지 좋은 지원을 받는 ONNX 양자화 형식을 추천합니다. 이 장에서는 GenAI를 위한 ONNX Runtime, OpenVINO, Apple MLX를 사용하여 모델 양자화를 수행할 것입니다(더 나은 방법이 있다면 PR을 제출하여 알려주세요).

**이 장에서는 다음 내용을 포함합니다**

1. [llama.cpp를 사용한 Phi-3.5 양자화](./021.UsingLlamacppQuantifyingPhi35.md)

2. [onnxruntime의 생성 AI 확장을 사용한 Phi-3.5 양자화](./022.UsingORTGenAIQuantifyingPhi35.md)

3. [Intel OpenVINO를 사용한 Phi-3.5 양자화](./023.UsingIntelOpenVINOQuantifyingPhi35.md)

4. [Apple MLX 프레임워크를 사용한 Phi-3.5 양자화](./024.UsingAppleMLXQuantifyingPhi35.md)

