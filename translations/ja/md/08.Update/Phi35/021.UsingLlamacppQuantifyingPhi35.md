# **Phi-3.5の量子化をllama.cppで行う**

## **llama.cppとは**

llama.cppは、主にC++で書かれたオープンソースのソフトウェアライブラリで、Llamaなどのさまざまな大規模言語モデル（LLM）に対して推論を行います。その主な目的は、幅広いハードウェアで最小限のセットアップで最先端のパフォーマンスを提供することです。また、このライブラリにはPythonバインディングもあり、テキスト補完やOpenAI互換のウェブサーバー用の高レベルAPIを提供します。

llama.cppの主な目標は、ローカルおよびクラウドのさまざまなハードウェア上で最小限のセットアップと最先端のパフォーマンスでLLM推論を可能にすることです。

- 依存関係のないプレーンなC/C++実装
- Appleシリコンに最適化されたARM NEON、Accelerate、およびMetalフレームワーク
- x86アーキテクチャ向けのAVX、AVX2、AVX512サポート
- 高速な推論とメモリ使用量の削減のための1.5ビット、2ビット、3ビット、4ビット、5ビット、6ビット、および8ビットの整数量子化
- NVIDIA GPUでLLMを実行するためのカスタムCUDAカーネル（HIPを介したAMD GPUサポート）
- VulkanおよびSYCLバックエンドのサポート
- VRAM容量を超えるモデルを部分的に加速するCPU+GPUハイブリッド推論

## **Phi-3.5をllama.cppで量子化する**

Phi-3.5-Instructモデルはllama.cppを使用して量子化できますが、Phi-3.5-VisionおよびPhi-3.5-MoEはまだサポートされていません。llama.cppで変換されたフォーマットはggufであり、これも最も広く使用されている量子化フォーマットです。

Hugging faceには大量の量子化されたGGUFフォーマットモデルがあります。AI Foundry、Ollama、およびLlamaEdgeはllama.cppに依存しているため、GGUFモデルもよく使用されます。

### **GGUFとは**

GGUFはモデルの高速な読み込みと保存に最適化されたバイナリフォーマットであり、推論目的において非常に効率的です。GGUFはGGMLや他のエグゼキューターと共に使用するために設計されています。GGUFはllama.cppの開発者である@ggerganovによって開発されました。最初にPyTorchなどのフレームワークで開発されたモデルは、これらのエンジンで使用するためにGGUFフォーマットに変換できます。

### **ONNX vs GGUF**

ONNXは伝統的な機械学習/深層学習フォーマットであり、さまざまなAIフレームワークでよくサポートされており、エッジデバイスでの使用シナリオに適しています。一方、GGUFはllama.cppに基づいており、GenAI時代に生み出されたと言えます。両者は似た用途を持ちます。組み込みハードウェアやアプリケーション層でより良いパフォーマンスを求めるなら、ONNXが選択肢となるでしょう。llama.cppの派生フレームワークや技術を使用するなら、GGUFが適しているかもしれません。

### **llama.cppを使用してPhi-3.5-Instructを量子化する**

**1. 環境設定**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. 量子化**

llama.cppを使用してPhi-3.5-InstructをFP16 GGUFに変換する


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Phi-3.5をINT4に量子化する


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. テスト**

llama-cpp-pythonをインストールする


```bash

pip install llama-cpp-python -U

```

***Note*** 

Apple Siliconを使用する場合は、次のようにllama-cpp-pythonをインストールしてください


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

テスト


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **リソース**

1. llama.cppについて詳しく知る [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. GGUFについて詳しく知る [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**免責事項**:
この文書は機械翻訳サービスを使用して翻訳されています。正確さを期すために努力していますが、自動翻訳には誤りや不正確さが含まれる可能性があります。元の言語での文書が権威ある情報源と見なされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤訳については責任を負いません。