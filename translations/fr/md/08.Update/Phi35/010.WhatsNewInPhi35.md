# **Quoi de neuf dans la famille Phi-3.5**

Utilisez-vous d√©j√† la famille Phi-3 ? Quels sont vos sc√©narios ? Le 20 ao√ªt 2024, Microsoft a lanc√© la nouvelle famille Phi-3.5, qui a √©t√© am√©lior√©e en multilingue, vision et Agent IA. Faisons une introduction plus d√©taill√©e en lien avec la Model Card sur Hugging Face.

![PhiFamily](../../../../../translated_images/Phi3getstarted.086dfb90bb69325da6b717586337f2aec5decc241fda85e322eb55c709167f73.fr.png)

## **Phi-3.5-mini-instruct**

Phi-3.5-mini est un mod√®le l√©ger et de pointe, construit √† partir de jeux de donn√©es utilis√©s pour Phi-3 - donn√©es synth√©tiques et sites web publics filtr√©s - en se concentrant sur des donn√©es de tr√®s haute qualit√© et denses en raisonnement. Le mod√®le appartient √† la famille des mod√®les Phi-3 et prend en charge une longueur de contexte de 128K tokens. Le mod√®le a subi un processus d'am√©lioration rigoureux, incorporant √† la fois un ajustement supervis√©, une optimisation de la politique proximale et une optimisation directe des pr√©f√©rences pour garantir une adh√©sion pr√©cise aux instructions et des mesures de s√©curit√© robustes.

![benchmark1](../../../../../translated_images/benchmark1.479cb048e7d9239b09e562c410a54f6c9eaf85030af67ac6e7de80a69e4778a5.fr.png)

![benchmark2](../../../../../translated_images/benchmark2.76982d411a07caa3ebd706dd6c0ba98b98a5609de371176a67cd619d70d4e6da.fr.png)

√Ä travers les indicateurs du Benchmark, vous pouvez voir que Phi-3.5-mini a am√©lior√© le support pour plusieurs langues et les contenus de texte long par rapport √† Phi-3-mini, ce qui est utilis√© pour am√©liorer les capacit√©s linguistiques et textuelles de Phi-3.5 mini dans les applications en p√©riph√©rie.

Nous pouvons comparer la capacit√© de connaissance en chinois √† travers les mod√®les GitHub. Lorsque nous demandons "O√π se trouve Changsha?" (ÈïøÊ≤ôÂú®Âì™Èáå?), nous pouvons comparer les r√©sultats de Phi-3-mini-128k-instruct et Phi-3.5-mini-128k-instruct.

![Phi3](../../../../../translated_images/gh3.6b1a5c38ed732e40c0effaf4c558badfab0be6148b194aa6bec44adbfb1e4342.fr.png)

![Phi35](../../../../../translated_images/gh35.b0fd2ff379a5f2d995ea1faedd2d7260cfcad7ffbad5a721a8a1b2b3d84028c8.fr.png)

Il n'est pas difficile de voir que l'am√©lioration des donn√©es sur le corpus chinois permet √† Phi-3.5-mini d'obtenir de meilleurs r√©sultats dans les sc√©narios de g√©n√©ration de texte de base (***Note:*** Veuillez noter que si Phi-3.5-mini a besoin d'une r√©ponse plus pr√©cise, il est recommand√© de le peaufiner selon le sc√©nario d'application).

## **Phi-3.5-vision-instruct**

Phi-3.5-vision est un mod√®le multimodal l√©ger et de pointe, construit √† partir de jeux de donn√©es qui incluent - donn√©es synth√©tiques et sites web publics filtr√©s - en se concentrant sur des donn√©es de tr√®s haute qualit√© et denses en raisonnement √† la fois sur le texte et la vision. Le mod√®le appartient √† la famille des mod√®les Phi-3, et la version multimodale vient avec une longueur de contexte de 128K tokens qu'il peut prendre en charge. Le mod√®le a subi un processus d'am√©lioration rigoureux, incorporant √† la fois un ajustement supervis√© et une optimisation directe des pr√©f√©rences pour garantir une adh√©sion pr√©cise aux instructions et des mesures de s√©curit√© robustes.

Gr√¢ce √† Vision, nous avons ouvert les yeux de la famille Phi-3.x et avons pu compl√©ter les sc√©narios suivants :

1. Environnements contraints en m√©moire/compute
2. Sc√©narios √† latence limit√©e
3. Compr√©hension g√©n√©rale des images
4. Reconnaissance optique de caract√®res
5. Compr√©hension des graphiques et des tableaux
6. Comparaison de plusieurs images
7. R√©sum√© de plusieurs images ou clips vid√©o

Gr√¢ce √† Vision, nous avons permis √† la famille Phi d'ouvrir ses yeux et de compl√©ter les sc√©narios suivants.

Nous pouvons √©galement utiliser le benchmark fourni par Hugging Face pour comprendre la comparaison dans diff√©rents sc√©narios visuels.

![benchmark3](../../../../../translated_images/benchmark3.4d9484cc062f0c5076783f3cb33fe533c03995d3a5debc437420e88960032672.fr.png)

Si vous souhaitez essayer l'essai gratuit de Phi-3.5-vision-instruct, nous pouvons utiliser [Nivida NIM](https://build.nvidia.com/microsoft/phi-3_5-vision-instruct) pour compl√©ter l'exp√©rience.

![nim](../../../../../translated_images/nim.c985945596d6b2629658087485d16028a3874dcc37329de51b94adf09d0af661.fr.png)

Bien s√ªr, vous pouvez √©galement compl√©ter le d√©ploiement via Azure AI Foundry.

## **Phi-3.5-MoE-instruct**

Phi-3.5-MoE est un mod√®le l√©ger et de pointe, construit √† partir de jeux de donn√©es utilis√©s pour Phi-3 - donn√©es synth√©tiques et documents publics filtr√©s - en se concentrant sur des donn√©es de tr√®s haute qualit√© et denses en raisonnement. Le mod√®le prend en charge plusieurs langues et vient avec une longueur de contexte de 128K tokens. Le mod√®le a subi un processus d'am√©lioration rigoureux, incorporant un ajustement supervis√©, une optimisation de la politique proximale et une optimisation directe des pr√©f√©rences pour garantir une adh√©sion pr√©cise aux instructions et des mesures de s√©curit√© robustes.

Avec le d√©veloppement de l'Agent IA, la demande de mod√®les MoE augmentera progressivement. MoE, dont le nom complet est Mixed Expert Models, est un nouveau mod√®le form√© en m√©langeant plusieurs mod√®les d'experts. MoE consiste √† diviser d'abord le gros probl√®me, puis √† r√©soudre les petits probl√®mes un par un, et ensuite √† r√©sumer les conclusions. Ensuite, l'√©chelle du mod√®le est l'un des facteurs cl√©s pour am√©liorer les performances du mod√®le. Avec des ressources de calcul limit√©es, il est souvent pr√©f√©rable de former un mod√®le plus grand avec moins d'√©tapes de formation que de former un mod√®le plus petit avec plus d'√©tapes.

Le mod√®le Phi-3.5-MoE-Instruct n√©cessite plus de puissance de calcul que Phi-3.5-Vision et Phi-3.5-Instruct. Il est recommand√© d'utiliser des m√©thodes bas√©es sur le cloud telles que Azure AI Foundry et Nvidia NIM pour l'exp√©rience et l'utilisation.

![nim2](../../../../../translated_images/nim2.ab50cc468e987efe5e87e8b9b2927f751b6d080c4a146129c2133da94b0f781e.fr.png)

### **ü§ñ Exemples pour Phi-3.5 avec Apple MLX**

| Labs    | Introduction | Go |
| -------- | ------- |  ------- |
| üöÄ Lab-Introduction Phi-3.5 Instruct  | Apprenez √† utiliser Phi-3.5 Instruct |  [Go](../../../../../code/09.UpdateSamples/Aug/phi3-instruct-demo.ipynb)    |
| üöÄ Lab-Introduction Phi-3.5 Vision (image) | Apprenez √† utiliser Phi-3.5 Vision pour analyser une image |  [Go](../../../../../code/09.UpdateSamples/Aug/phi3-vision-demo.ipynb)    |
| üöÄ Lab-Introduction Phi-3.5 MoE   | Apprenez √† utiliser Phi-3.5 Vision pour analyser une image |  [Go](../../../../../code/09.UpdateSamples/Aug/phi3_moe_demo.ipynb)    |

## **Ressources**

1. La famille Phi de Hugging Face [https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)

2. √Ä propos des mod√®les GitHub [https://gh.io/models](https://gh.io/models)

3. √Ä propos de Azure AI Foundry [https://ai.azure.com/](https://ai.azure.com/)

4. √Ä propos de Nvidia NIM [https://build.nvidia.com/explore/discover](https://build.nvidia.com/explore/discover)

**Avertissement**:
Ce document a √©t√© traduit en utilisant des services de traduction automatis√©e par IA. Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.