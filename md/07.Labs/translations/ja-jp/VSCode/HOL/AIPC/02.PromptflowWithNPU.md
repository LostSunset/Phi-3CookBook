# ラボ 2 - AIPCでPhi-3-miniのPrompt flowを実行する

## **Prompt flowとは**

Prompt flowは、LLM（大規模言語モデル）ベースのAIアプリケーションのアイデアからプロトタイプ、テスト、評価、プロダクションデプロイメント、監視までのエンドツーエンドの開発サイクルを簡素化するための開発ツールセットです。これにより、プロンプトエンジニアリングが大幅に簡素化され、生産品質のLLMアプリケーションを構築することができます。

Prompt flowを使用すると、次のことが可能になります：

- LLM、プロンプト、Pythonコード、およびその他のツールをリンクする実行可能なワークフローを作成します。
- 特にLLMとの対話において、ワークフローを簡単にデバッグおよび反復します。
- ワークフローを評価し、より大きなデータセットを使用して品質とパフォーマンスの指標を計算します。
- ワークフローの品質を確保するために、テストと評価をCI/CDシステムに統合します。
- ワークフローを選択したサービスプラットフォームにデプロイするか、アプリケーションコードベースに簡単に統合します。
- （オプションですが強く推奨）Azure AIのPrompt flowクラウドバージョンを利用してチームと協力します。

## AIPCとは

AI PCは、CPU、GPU、およびNPUを備えており、それぞれが特定のAIアクセラレーション機能を持っています。NPU（ニューラルプロセッシングユニット）は、データをクラウドに送信して処理するのではなく、PC上でAIおよび機械学習タスクを直接処理する専用のアクセラレータです。GPUおよびCPUもこれらのワークロードを処理できますが、NPUは特に低電力AI計算に優れています。AI PCは、私たちのコンピュータ操作方法に根本的な変革をもたらします。以前には存在しなかった問題を解決するのではなく、日常のPC使用に大幅な改善をもたらします。

それでは、どのように機能するのでしょうか？大量の公共データに基づいてトレーニングされた大規模言語モデル（LLM）とは異なり、PC上で実行されるAIは、あらゆる面でよりアクセスしやすくなります。この概念は理解しやすく、データに基づいてトレーニングされているため、クラウドへのアクセスが不要で、より広範な人々にとって魅力的です。

近い将来、AI PCは、PC上で直接実行される個人アシスタントや小型のAIモデルを提供し、日常のタスクに対して個人的でプライベートかつ安全なAIエンハンスメントを提供します。これには、会議のメモを取る、スポーツイベントを整理する、写真やビデオ編集を自動的に強化する、または各人の到着および出発時間に基づいて完璧な家族の集まりのスケジュールを作成することが含まれます。

## AIPC上でコード生成ワークフローを構築する

***注意***：環境のインストールがまだ完了していない場合は、[ラボ 0 - インストール](./01.Installations.md)にアクセスしてください。

1. Visual Studio CodeでPrompt flow拡張機能を開き、空のワークフロープロジェクトを作成します。

![create](../../../../../../../imgs/07/01/pf_create.png)

2. 入力および出力パラメータを追加し、Pythonコードを新しいワークフローとして追加します。

![flow](../../../../../../../imgs/07/01/pf_flow.png)

次の構造（flow.dag.yaml）を参考にしてワークフローを構築できます。

```yaml
inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}
```

3. `Chat_With_Phi3.py`にコードを追加します。

```python
from promptflow.core import tool

# import torch
from transformers import AutoTokenizer, pipeline, TextStreamer
import intel_npu_acceleration_library as npu_lib

import warnings

import asyncio
import platform

class Phi3CodeAgent:

    model = None
    tokenizer = None
    text_streamer = None

    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():

        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                                    Phi3CodeAgent.model_id,
                                    torch_dtype="auto",
                                    dtype=npu_lib.int4,
                                    trust_remote_code=True
                                )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)



    @staticmethod
    def chat_with_phi3(prompt):

        Phi3CodeAgent.init_phi3()

        messages = "<|system|>あなたはAI Pythonコーディングアシスタントです。Pythonコードを生成するのを手伝ってください。回答はPythonコードのみを生成し、コメントや説明は不要です<|end|><|user|>" + prompt +"<|end|><|assistant|>"

        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
            # **generation_args
        )

        result = ''

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result = response[0]['generated_text']
            return result

@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)
```

4. ワークフローをデバッグまたは実行して、生成されたコードが正しいかどうかを確認できます。

![RUN](../../../../../../../imgs/07/01/pf_run.png)

5. ターミナルでワークフローを開発APIとして実行します。

```
pf flow serve --source ./ --port 8080 --host localhost
```

Postman / Thunder Clientでテストできます。

**注意**

1. 初回の実行には時間がかかります。Hugging face CLIを使用してphi-3モデルをダウンロードすることをお勧めします。
2. Intel NPUの計算能力が限られているため、Phi-3-mini-4k-instructを使用することをお勧めします。
3. INT4変換量子化のためにIntel NPUアクセラレーションを使用していますが、サービスを再実行する場合は、キャッシュとnc_workshopフォルダーを削除する必要があります。

## **リソース**

1. Promptflowを学ぶ [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)
2. Intel NPUアクセラレーションを学ぶ [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)
3. サンプルコード、[ローカルNPUエージェントのサンプルコードをダウンロード](../../../../../../../code/07.Lab/translations/zh-cn//01/AIPC/local-npu-agent/)
