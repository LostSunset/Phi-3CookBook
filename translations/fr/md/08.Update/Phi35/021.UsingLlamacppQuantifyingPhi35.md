# **Quantification de Phi-3.5 avec llama.cpp**

## **Qu'est-ce que llama.cpp**

llama.cpp est une bibliothèque logicielle open-source principalement écrite en C++ qui effectue des inférences sur divers grands modèles de langage (LLMs), tels que Llama. Son objectif principal est de fournir des performances de pointe pour l'inférence LLM sur une large gamme de matériel avec une configuration minimale. De plus, il existe des liaisons Python disponibles pour cette bibliothèque, offrant une API de haut niveau pour la complétion de texte et un serveur web compatible avec OpenAI.

Le principal objectif de llama.cpp est de permettre l'inférence LLM avec une configuration minimale et des performances de pointe sur une grande variété de matériel - localement et dans le cloud.

- Implémentation en C/C++ sans dépendances
- Apple silicon est un citoyen de première classe - optimisé via les frameworks ARM NEON, Accelerate et Metal
- Support AVX, AVX2 et AVX512 pour les architectures x86
- Quantification en entiers 1,5 bits, 2 bits, 3 bits, 4 bits, 5 bits, 6 bits et 8 bits pour une inférence plus rapide et une utilisation réduite de la mémoire
- Noyaux CUDA personnalisés pour exécuter des LLMs sur des GPU NVIDIA (support pour les GPU AMD via HIP)
- Support des backends Vulkan et SYCL
- Inférence hybride CPU+GPU pour accélérer partiellement les modèles plus grands que la capacité totale de VRAM

## **Quantification de Phi-3.5 avec llama.cpp**

Le modèle Phi-3.5-Instruct peut être quantifié en utilisant llama.cpp, mais Phi-3.5-Vision et Phi-3.5-MoE ne sont pas encore supportés. Le format converti par llama.cpp est gguf, qui est également le format de quantification le plus largement utilisé.

Il existe un grand nombre de modèles au format GGUF quantifiés sur Hugging face. AI Studio, Ollama et LlamaEdge s'appuient sur llama.cpp, donc les modèles GGUF sont également souvent utilisés.

### **Qu'est-ce que GGUF**

GGUF est un format binaire optimisé pour le chargement et la sauvegarde rapides des modèles, le rendant très efficace pour les inférences. GGUF est conçu pour être utilisé avec GGML et d'autres exécutants. GGUF a été développé par @ggerganov qui est également le développeur de llama.cpp, un cadre d'inférence LLM populaire en C/C++. Les modèles initialement développés dans des frameworks comme PyTorch peuvent être convertis au format GGUF pour être utilisés avec ces moteurs.

### **ONNX vs GGUF**

ONNX est un format traditionnel de machine learning/deep learning, bien supporté dans différents frameworks AI et ayant de bons scénarios d'utilisation dans les dispositifs embarqués. Quant à GGUF, il est basé sur llama.cpp et peut être considéré comme produit à l'ère du GenAI. Les deux ont des usages similaires. Si vous souhaitez de meilleures performances dans le matériel embarqué et les couches d'application, ONNX peut être votre choix. Si vous utilisez le cadre dérivé et la technologie de llama.cpp, alors GGUF peut être mieux.

### **Quantification de Phi-3.5-Instruct en utilisant llama.cpp**

**1. Configuration de l'environnement**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. Quantification**

Utiliser llama.cpp pour convertir Phi-3.5-Instruct en FP16 GGUF


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Quantification de Phi-3.5 en INT4


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. Test**

Installer llama-cpp-python


```bash

pip install llama-cpp-python -U

```

***Note*** 

Si vous utilisez Apple Silicon, veuillez installer llama-cpp-python comme ceci


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

Test 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nPeux-tu présenter .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **Ressources**

1. En savoir plus sur llama.cpp [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. En savoir plus sur GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

Avertissement : La traduction a été réalisée à partir de l'original par un modèle d'IA et peut ne pas être parfaite. 
Veuillez examiner le résultat et apporter les corrections nécessaires.