# **onnxruntime の生成 AI 拡張機能を使用した Phi-3.5 の量子化**

## **onnxruntime の生成 AI 拡張機能とは**

この拡張機能を使用すると、ONNX Runtime で生成 AI を実行できます ( [https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai) )。ONNX モデルの生成 AI ループを提供し、ONNX Runtime を使用した推論、ロジット処理、検索とサンプリング、KV キャッシュ管理などを行います。開発者は高レベルの generate() メソッドを呼び出すか、ループ内でモデルの各反復を実行し、1 トークンずつ生成し、オプションでループ内の生成パラメータを更新できます。貪欲探索/ビーム探索および TopP、TopK サンプリングをサポートしており、トークンシーケンスを生成し、繰り返しペナルティのような組み込みのロジット処理を行います。カスタムスコアリングも簡単に追加できます。

アプリケーションレベルでは、onnxruntime の生成 AI 拡張機能を使用して C++/C#/Python を使用してアプリケーションを構築できます。モデルレベルでは、微調整されたモデルをマージしたり、関連する定量的なデプロイ作業を行うことができます。


## **onnxruntime の生成 AI 拡張機能を使用した Phi-3.5 の量子化**

### **サポートモデル**

onnxruntime の生成 AI 拡張機能は、Microsoft Phi、Google Gemma、Mistral、Meta LLaMA の量子化変換をサポートしています。


### **onnxruntime の生成 AI 拡張機能のモデルビルダー**

モデルビルダーは、ONNX Runtime generate() API で実行される最適化および量子化された ONNX モデルの作成を大幅に加速します。

モデルビルダーを使用すると、モデルを INT4、INT8、FP16、FP32 に量子化し、CPU、CUDA、DirectML、モバイルなどの異なるハードウェアアクセラレーション方法を組み合わせることができます。

モデルビルダーを使用するには、以下をインストールする必要があります

```bash

pip install torch transformers onnx onnxruntime

pip install --pre onnxruntime-genai

```

インストール後、ターミナルからモデルビルダースクリプトを実行して、モデルの形式と量子化変換を行うことができます。


```bash

python3 -m onnxruntime_genai.models.builder -m model_name -o path_to_output_folder -p precision -e execution_provider -c cache_dir_to_save_hf_files

```

関連するパラメータを理解する

1. **model_name** これは Hugging face 上のモデルで、例えば microsoft/Phi-3.5-mini-instruct、microsoft/Phi-3.5-vision-instruct などです。モデルを保存しているパスでもかまいません。

2. **path_to_output_folder** 量子化変換の保存パス

3. **execution_provider** 異なるハードウェアアクセラレーションのサポート、例えば cpu、cuda、DirectML

4. **cache_dir_to_save_hf_files** Hugging face からモデルをダウンロードし、ローカルにキャッシュします




***注意：***
2023年10月までのデータで訓練されています。

## **Phi-3.5 を量子化するためのモデルビルダーの使用方法**

モデルビルダーは現在、Phi-3.5 Instruct と Phi-3.5-Vision の ONNX モデル量子化をサポートしています

### **Phi-3.5-Instruct**


**CPU による INT 4 の量子化変換**

```bash

python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cpu -c ./Phi-3.5-mini-instruct

```

**CUDA による INT 4 の量子化変換**

```bash

python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct

```



```python

python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct

```


### **Phi-3.5-Vision**

**Phi-3.5-vision-instruct-onnx-cpu-fp32**

1. ターミナルで環境を設定する

```bash

mkdir models

cd models 

```

2. models フォルダに microsoft/Phi-3.5-vision-instruct をダウンロードする
[https://huggingface.co/microsoft/Phi-3.5-vision-instruct](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)

3. これらのファイルを Phi-3.5-vision-instruct フォルダにダウンロードしてください

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py)


4. このファイルを models フォルダにダウンロードしてください
[https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py)

5. ターミナルに移動

    ONNX サポートを FP32 で変換する


```bash

python build.py -i .\Your Phi-3.5-vision-instruct Path\ -o .\vision-cpu-fp32 -p f32 -e cpu

```


### **注意：**

1. モデルビルダーは現在、Phi-3.5-Instruct と Phi-3.5-Vision の変換をサポートしていますが、Phi-3.5-MoE はサポートしていません

2. ONNX の量子化モデルを使用するには、onnxruntime SDK の生成 AI 拡張機能を通じて使用できます

3. より責任ある AI を考慮する必要があるため、モデルの量子化変換後は、より効果的な結果テストを行うことをお勧めします

4. CPU INT4 モデルを量子化することで、エッジデバイスにデプロイすることができ、より良いアプリケーションシナリオを持つことができるため、Phi-3.5-Instruct の INT 4 を完了しました


## **リソース**

1. onnxruntime の生成 AI 拡張機能について詳しく知る [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. onnxruntime の生成 AI 拡張機能の GitHub リポジトリ [https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai)

**免責事項**：
この文書は、機械ベースのAI翻訳サービスを使用して翻訳されています。私たちは正確性を追求していますが、自動翻訳には誤りや不正確さが含まれる場合がありますのでご注意ください。元の言語で記載された原文が信頼できる情報源とみなされるべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用に起因する誤解や誤解について、当社は一切の責任を負いません。