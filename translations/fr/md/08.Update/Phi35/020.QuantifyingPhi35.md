# **Quantification de la Famille Phi-3.5**

La quantification des modèles fait référence au processus de mappage des paramètres (tels que les poids et les valeurs d'activation) dans un modèle de réseau de neurones d'une large gamme de valeurs (généralement une gamme de valeurs continues) à une gamme de valeurs plus petite et finie. Cette technologie peut réduire la taille et la complexité computationnelle du modèle et améliorer l'efficacité opérationnelle du modèle dans des environnements aux ressources limitées tels que les appareils mobiles ou les systèmes embarqués. La quantification des modèles permet de compresser en réduisant la précision des paramètres, mais elle introduit également une certaine perte de précision. Par conséquent, dans le processus de quantification, il est nécessaire de trouver un équilibre entre la taille du modèle, la complexité computationnelle et la précision. Les méthodes de quantification courantes incluent la quantification en virgule fixe, la quantification en virgule flottante, etc. Vous pouvez choisir la stratégie de quantification appropriée en fonction du scénario et des besoins spécifiques.

Nous espérons déployer le modèle GenAI sur des appareils en périphérie et permettre à plus d'appareils d'entrer dans des scénarios GenAI, tels que les appareils mobiles, les PC AI / Copilot+PC et les appareils IoT traditionnels. Grâce au modèle de quantification, nous pouvons le déployer sur différents appareils en périphérie en fonction des différents appareils. Combiné avec le cadre d'accélération de modèle et le modèle de quantification fournis par les fabricants de matériel, nous pouvons créer de meilleurs scénarios d'application SLM.

Dans le scénario de quantification, nous avons différentes précisions (INT4, INT8, FP16, FP32). Voici une explication des précisions de quantification couramment utilisées :

### **INT4**

La quantification INT4 est une méthode de quantification radicale qui quantifie les poids et les valeurs d'activation du modèle en entiers de 4 bits. La quantification INT4 entraîne généralement une perte de précision plus importante en raison de la gamme de représentation plus petite et de la précision inférieure. Cependant, par rapport à la quantification INT8, la quantification INT4 peut encore réduire les exigences de stockage et la complexité computationnelle du modèle. Il convient de noter que la quantification INT4 est relativement rare dans les applications pratiques, car une précision trop faible peut entraîner une dégradation significative des performances du modèle. De plus, tous les matériels ne supportent pas les opérations INT4, il est donc nécessaire de prendre en compte la compatibilité matérielle lors du choix d'une méthode de quantification.

### **INT8**

La quantification INT8 est le processus de conversion des poids et des activations d'un modèle de nombres en virgule flottante en entiers de 8 bits. Bien que la gamme de valeurs représentée par les entiers INT8 soit plus petite et moins précise, elle peut réduire considérablement les exigences de stockage et de calcul. Dans la quantification INT8, les poids et les valeurs d'activation du modèle passent par un processus de quantification, y compris l'échelle et le décalage, pour préserver autant que possible les informations originales en virgule flottante. Lors de l'inférence, ces valeurs quantifiées seront déquantifiées en nombres en virgule flottante pour le calcul, puis quantifiées en INT8 pour l'étape suivante. Cette méthode peut fournir une précision suffisante dans la plupart des applications tout en maintenant une efficacité computationnelle élevée.

### **FP16**

Le format FP16, c'est-à-dire les nombres en virgule flottante de 16 bits (float16), réduit l'empreinte mémoire de moitié par rapport aux nombres en virgule flottante de 32 bits (float32), ce qui présente des avantages significatifs dans les applications de deep learning à grande échelle. Le format FP16 permet de charger des modèles plus grands ou de traiter plus de données dans les mêmes limitations de mémoire GPU. Alors que le matériel GPU moderne continue de prendre en charge les opérations FP16, l'utilisation du format FP16 peut également apporter des améliorations en termes de vitesse de calcul. Cependant, le format FP16 présente également des inconvénients inhérents, à savoir une précision inférieure, ce qui peut entraîner une instabilité numérique ou une perte de précision dans certains cas.

### **FP32**

Le format FP32 offre une précision plus élevée et peut représenter avec précision une large gamme de valeurs. Dans les scénarios où des opérations mathématiques complexes sont effectuées ou des résultats de haute précision sont requis, le format FP32 est préféré. Cependant, une haute précision signifie également une utilisation de mémoire plus importante et un temps de calcul plus long. Pour les modèles de deep learning à grande échelle, en particulier lorsqu'il y a de nombreux paramètres de modèle et une quantité énorme de données, le format FP32 peut entraîner une insuffisance de mémoire GPU ou une diminution de la vitesse d'inférence.

Sur les appareils mobiles ou les appareils IoT, nous pouvons convertir les modèles Phi-3.x en INT4, tandis que les PC AI / Copilot PC peuvent utiliser une précision plus élevée telle que INT8, FP16, FP32.

À l'heure actuelle, différents fabricants de matériel disposent de différents cadres pour prendre en charge les modèles génératifs, tels que OpenVINO d'Intel, QNN de Qualcomm, MLX d'Apple et CUDA de Nvidia, etc., combinés avec la quantification des modèles pour compléter le déploiement local.

En termes de technologie, nous avons différents supports de format après quantification, tels que le format PyTorch / Tensorflow, GGUF et ONNX. J'ai fait une comparaison de formats et des scénarios d'application entre GGUF et ONNX. Ici, je recommande le format de quantification ONNX, qui bénéficie d'un bon support du cadre de modèle au matériel. Dans ce chapitre, nous nous concentrerons sur ONNX Runtime pour GenAI, OpenVINO et Apple MLX pour effectuer la quantification des modèles (si vous avez une meilleure méthode, vous pouvez également nous la soumettre en envoyant une PR).

**Ce chapitre inclut**

1. [Quantification de Phi-3.5 en utilisant llama.cpp](./021.UsingLlamacppQuantifyingPhi35.md)

2. [Quantification de Phi-3.5 en utilisant les extensions Generative AI pour onnxruntime](./022.UsingORTGenAIQuantifyingPhi35.md)

3. [Quantification de Phi-3.5 en utilisant Intel OpenVINO](./023.UsingIntelOpenVINOQuantifyingPhi35.md)

4. [Quantification de Phi-3.5 en utilisant le Framework Apple MLX](./024.UsingAppleMLXQuantifyingPhi35.md)

**Avertissement**:
Ce document a été traduit en utilisant des services de traduction automatique basés sur l'IA. Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de faire appel à une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interprétations erronées résultant de l'utilisation de cette traduction.