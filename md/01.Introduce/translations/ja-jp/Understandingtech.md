## 言及された主要技術には以下が含まれます：

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12上に構築されたハードウェアアクセラレート機械学習のための低レベルAPI。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidiaによって開発された並列計算プラットフォームおよびアプリケーションプログラミングインターフェース（API）モデルで、グラフィックス処理ユニット（GPU）上での汎用処理を可能にします。
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - 機械学習モデルを表現するために設計されたオープンフォーマットで、異なるMLフレームワーク間の相互運用性を提供します。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - 機械学習モデルを表現および更新するために使用されるフォーマットで、特に4-8ビット量子化されたCPU上で効果的に動作する小型言語モデルに役立ちます。

## DirectML

DirectMLは、ハードウェアアクセラレート機械学習を可能にする低レベルAPIです。これはDirectX 12上に構築されており、GPUアクセラレーションを利用し、ベンダーに依存しないため、異なるGPUベンダー間でコードの変更を必要としません。主にGPU上でのモデルトレーニングおよび推論ワークロードに使用されます。

ハードウェアサポートに関しては、DirectMLはAMD統合およびディスクリートGPU、Intel統合GPU、およびNVIDIAディスクリートGPUを含む幅広いGPUと連携するように設計されています。これはWindows AIプラットフォームの一部であり、Windows 10および11でサポートされており、任意のWindowsデバイスでのモデルトレーニングおよび推論を可能にします。

DirectMLに関連する更新および機会には、150個のONNXオペレーターのサポートや、ONNXランタイムおよびWinMLの両方で使用されることが含まれます。これは主要な統合ハードウェアベンダー（IHV）によってサポートされており、各ベンダーはさまざまなメタコマンドを実装しています。

## CUDA

CUDA（Compute Unified Device Architectureの略）は、Nvidiaによって作成された並列計算プラットフォームおよびアプリケーションプログラミングインターフェース（API）モデルです。これにより、ソフトウェア開発者はCUDA対応のグラフィックス処理ユニット（GPU）を使用して汎用処理を行うことができます。このアプローチはGPGPU（Graphics Processing Units上の汎用計算）と呼ばれます。CUDAはNvidiaのGPUアクセラレーションの主要な推進力であり、機械学習、科学計算、ビデオ処理などのさまざまな分野で広く使用されています。

CUDAのハードウェアサポートは、NvidiaのGPUに特化しており、これはNvidiaによって開発された専有技術です。各アーキテクチャは特定のバージョンのCUDAツールキットをサポートしており、開発者がCUDAアプリケーションを構築および実行するために必要なライブラリおよびツールを提供します。

## ONNX

ONNX（Open Neural Network Exchange）は、機械学習モデルを表現するために設計されたオープンフォーマットです。これにより、拡張可能な計算グラフモデルの定義、および組み込みオペレーターと標準データ型の定義が提供されます。ONNXは、開発者が異なるMLフレームワーク間でモデルを移動させることを可能にし、相互運用性を実現し、AIアプリケーションの作成およびデプロイを容易にします。

Phi3 miniは、サーバープラットフォーム、Windows、Linux、Macデスクトップ、およびモバイルCPUを含むデバイス全体で、CPUおよびGPU上でONNXランタイムを使用して実行できます。
最適化された構成には以下が含まれます：

- int4 DML用のONNXモデル：AWQを介してint4に量子化
- fp16 CUDA用のONNXモデル
- int4 CUDA用のONNXモデル：RTNを介してint4に量子化
- int4 CPUおよびモバイル用のONNXモデル：RTNを介してint4に量子化

## Llama.cpp

Llama.cppは、C++で記述されたオープンソースのソフトウェアライブラリです。これは、Llamaを含むさまざまな大型言語モデル（LLM）で推論を行います。ggmlライブラリ（汎用テンソルライブラリ）と共同で開発され、llama.cppは元のPython実装と比較して高速な推論と低いメモリ使用量を提供することを目的としています。これはハードウェア最適化、量子化をサポートし、シンプルなAPIとサンプルを提供します。効率的なLLM推論に興味がある場合、llama.cppは探索する価値があります。Phi3はLlama.cppを実行できます。

## GGUF

GGUF（Generic Graph Update Format）は、機械学習モデルを表現および更新するために使用されるフォーマットです。これは、4-8ビット量子化されたCPU上で効果的に動作する小型言語モデル（SLM）に特に役立ちます。GGUFは、エッジデバイス上での迅速なプロトタイピングおよびCI/CDパイプラインのようなバッチジョブでのモデル実行に有益です。
