# **onnxruntime의 Generative AI 확장을 사용하여 Phi-3.5 양자화하기**

## **onnxruntime의 Generative AI 확장이란?**

이 확장은 ONNX Runtime( [https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai))을 사용하여 생성 AI를 실행할 수 있도록 도와줍니다. ONNX 모델에 대한 생성 AI 루프를 제공하며, ONNX Runtime을 사용한 추론, 로짓 처리, 검색 및 샘플링, KV 캐시 관리 등을 포함합니다. 개발자는 고수준의 generate() 메서드를 호출하거나 모델의 각 반복을 루프에서 실행하여 한 번에 하나의 토큰을 생성하고, 루프 내부에서 생성 매개변수를 선택적으로 업데이트할 수 있습니다. 그리디/빔 검색 및 TopP, TopK 샘플링을 지원하여 토큰 시퀀스를 생성하며, 반복 페널티와 같은 내장된 로짓 처리를 제공합니다. 또한 사용자 정의 점수를 쉽게 추가할 수 있습니다.

애플리케이션 수준에서는 onnxruntime의 Generative AI 확장을 사용하여 C++/ C# / Python을 사용한 애플리케이션을 구축할 수 있습니다. 모델 수준에서는 세부 조정된 모델을 병합하고 관련 양자화 배포 작업을 수행할 수 있습니다.


## **onnxruntime의 Generative AI 확장을 사용하여 Phi-3.5 양자화하기**

### **지원 모델**

onnxruntime의 Generative AI 확장은 Microsoft Phi, Google Gemma, Mistral, Meta LLaMA의 양자화 변환을 지원합니다.


### **onnxruntime의 Generative AI 확장의 모델 빌더**

모델 빌더는 ONNX Runtime generate() API로 실행되는 최적화되고 양자화된 ONNX 모델을 만드는 속도를 크게 높입니다.

모델 빌더를 통해 모델을 INT4, INT8, FP16, FP32로 양자화할 수 있으며, CPU, CUDA, DirectML, Mobile 등의 다양한 하드웨어 가속 방법을 결합할 수 있습니다.

모델 빌더를 사용하려면 다음을 설치해야 합니다:

```bash
pip install torch transformers onnx onnxruntime
pip install --pre onnxruntime-genai
```

설치 후, 터미널에서 모델 빌더 스크립트를 실행하여 모델 형식 및 양자화 변환을 수행할 수 있습니다.

```bash
python3 -m onnxruntime_genai.models.builder -m model_name -o path_to_output_folder -p precision -e execution_provider -c cache_dir_to_save_hf_files
```

관련 매개변수 이해하기

1. **model_name**: Hugging Face의 모델 이름, 예를 들어 microsoft/Phi-3.5-mini-instruct, microsoft/Phi-3.5-vision-instruct 등. 모델을 저장한 경로일 수도 있습니다.

2. **path_to_output_folder**: 양자화 변환 저장 경로

3. **execution_provider**: CPU, CUDA, DirectML 등의 하드웨어 가속 지원

4. **cache_dir_to_save_hf_files**: Hugging Face에서 모델을 다운로드하고 로컬에 캐시합니다.



***참고:***

## **모델 빌더를 사용하여 Phi-3.5 양자화하는 방법**

모델 빌더는 이제 Phi-3.5 Instruct와 Phi-3.5-Vision의 ONNX 모델 양자화를 지원합니다.

### **Phi-3.5-Instruct**

**양자화된 INT 4의 CPU 가속 변환**

```bash
python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cpu -c ./Phi-3.5-mini-instruct
```

**양자화된 INT 4의 CUDA 가속 변환**

```bash
python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct
```

```python
python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct
```

### **Phi-3.5-Vision**

**Phi-3.5-vision-instruct-onnx-cpu-fp32**

1. 터미널에서 환경 설정

```bash
mkdir models
cd models 
```

2. models 폴더에 microsoft/Phi-3.5-vision-instruct 다운로드
[https://huggingface.co/microsoft/Phi-3.5-vision-instruct](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)

3. 다음 파일들을 Phi-3.5-vision-instruct 폴더에 다운로드

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py)

4. 이 파일을 models 폴더에 다운로드
[https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py)

5. 터미널로 이동

    FP32를 사용한 ONNX 지원으로 변환

```bash
python build.py -i .\Your Phi-3.5-vision-instruct Path\ -o .\vision-cpu-fp32 -p f32 -e cpu
```

### **참고:**

1. 모델 빌더는 현재 Phi-3.5-Instruct와 Phi-3.5-Vision의 변환을 지원하지만, Phi-3.5-MoE는 지원하지 않습니다.

2. ONNX의 양자화된 모델을 사용하려면 onnxruntime SDK를 통한 Generative AI 확장을 사용해야 합니다.

3. 더 책임감 있는 AI를 고려해야 하므로 모델 양자화 변환 후에는 더 효과적인 결과 테스트를 권장합니다.

4. CPU INT4 모델을 양자화함으로써 Edge Device에 배포할 수 있으며, 더 나은 응용 시나리오를 제공합니다. 따라서 INT 4를 중심으로 한 Phi-3.5-Instruct를 완료했습니다.


## **리소스**

1. onnxruntime의 Generative AI 확장에 대해 더 알아보기 [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. onnxruntime의 Generative AI 확장 GitHub 저장소 [https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai)

면책 조항: 이 번역은 원본을 AI 모델에 의해 번역된 것으로 완벽하지 않을 수 있습니다. 
출력을 검토하고 필요한 수정 사항을 반영해 주시기 바랍니다.