# **Phi-3.5 ファミリーの新機能**

Phi-3 ファミリーをすでに利用していますか？どのようなシナリオで使用していますか？2024年8月20日、Microsoftは多言語対応、ビジョン、AIエージェントが強化された新しいPhi-3.5ファミリーをリリースしました。Hugging faceのモデルカードと併せて、より詳細な紹介を行います。

![PhiFamily](../../../../../translated_images/Phi3getstarted.086dfb90bb69325da6b717586337f2aec5decc241fda85e322eb55c709167f73.ja.png)


## **Phi-3.5-mini-instruct**

Phi-3.5-miniは、Phi-3で使用されたデータセット（合成データとフィルタリングされた公開ウェブサイト）を基に構築された、軽量で最先端のオープンモデルです。非常に高品質で推論が密なデータに重点を置いています。このモデルはPhi-3モデルファミリーに属し、128Kトークンのコンテキスト長をサポートします。モデルは厳格な強化プロセスを経て、監督付きファインチューニング、近接ポリシー最適化、および直接の嗜好最適化を組み込んで、正確な指示の遵守と堅牢な安全対策を確保しています。

![benchmark1](../../../../../translated_images/benchmark1.479cb048e7d9239b09e562c410a54f6c9eaf85030af67ac6e7de80a69e4778a5.ja.png)

![benchmark2](../../../../../translated_images/benchmark2.76982d411a07caa3ebd706dd6c0ba98b98a5609de371176a67cd619d70d4e6da.ja.png)

ベンチマークの指標を見ると、Phi-3-miniと比較して、Phi-3.5-miniは多言語対応と長文コンテンツのサポートが向上していることがわかります。これは、エッジアプリケーションにおけるPhi-3.5 miniの言語とテキスト能力を強化するためです。

GitHub Modelsを使用して中国語の知識の能力を比較することができます。「Changshaはどこにありますか？」（长沙在哪里？）と尋ねると、Phi-3-mini-128k-instructとPhi-3.5-mini-128k-instructの結果を比較できます。

![Phi3](../../../../../translated_images/gh3.6b1a5c38ed732e40c0effaf4c558badfab0be6148b194aa6bec44adbfb1e4342.ja.png)

![Phi35](../../../../../translated_images/gh35.b0fd2ff379a5f2d995ea1faedd2d7260cfcad7ffbad5a721a8a1b2b3d84028c8.ja.png)

中国語コーパスのデータ強化により、Phi-3.5-miniが基本的なテキスト生成シナリオでより良い結果を得ることができることは明らかです（***Note:*** Phi-3.5-miniがより正確な回答を必要とする場合、アプリケーションシナリオに応じてファインチューニングすることをお勧めします）

## **Phi-3.5-vision-instruct**

Phi-3.5-visionは、合成データとフィルタリングされた公開ウェブサイトを含むデータセットを基に構築された、軽量で最先端のオープンマルチモーダルモデルです。非常に高品質で推論が密なデータに重点を置いています。このモデルはPhi-3モデルファミリーに属し、マルチモーダルバージョンは128Kトークンのコンテキスト長をサポートします。モデルは厳格な強化プロセスを経て、監督付きファインチューニングと直接の嗜好最適化を組み込んで、正確な指示の遵守と堅牢な安全対策を確保しています。

Visionを通じてPhi-3.xファミリーの目を開き、次のシナリオを完了することができました。

1. メモリ/コンピュートが制約された環境
2. レイテンシーが制約されたシナリオ
3. 一般的な画像理解
4. 光学文字認識
5. チャートとテーブルの理解
6. 複数の画像比較
7. 複数画像やビデオクリップの要約

Visionを通じて、Phiファミリーの目を開き、次のシナリオを完了します。

提供されているHugging faceのベンチマークを使用して、さまざまなビジュアルシナリオでの比較を理解することもできます。

![benchmark3](../../../../../translated_images/benchmark3.4d9484cc062f0c5076783f3cb33fe533c03995d3a5debc437420e88960032672.ja.png)

Phi-3.5-vision-instructの無料トライアルを試したい場合は、[Nivida NIM](https://build.nvidia.com/microsoft/phi-3_5-vision-instruct)を使用して体験を完了することができます。

![nim](../../../../../translated_images/nim.c985945596d6b2629658087485d16028a3874dcc37329de51b94adf09d0af661.ja.png)

もちろん、Azure AI Foundryを通じてデプロイを完了することもできます。

## **Phi-3.5-MoE-instruct**

Phi-3.5-MoEは、Phi-3で使用されたデータセット（合成データとフィルタリングされた公開文書）を基に構築された、軽量で最先端のオープンモデルです。非常に高品質で推論が密なデータに重点を置いています。モデルは多言語対応であり、128Kトークンのコンテキスト長をサポートします。モデルは厳格な強化プロセスを経て、監督付きファインチューニング、近接ポリシー最適化、および直接の嗜好最適化を組み込んで、正確な指示の遵守と堅牢な安全対策を確保しています。

AIエージェントの発展に伴い、MoEモデルの需要は徐々に増加します。MoEの正式名称はMixed Expert Modelsであり、複数の専門家モデルを混ぜ合わせた新しいモデルです。MOEは大きな問題をまず分割し、小さな問題を一つずつ解決し、結論をまとめるというものです。第二に、モデルの規模はモデルのパフォーマンスを向上させるための重要な要素の一つです。限られたコンピューティングリソースでは、少ないトレーニングステップでより大きなモデルをトレーニングする方が、より多くのステップで小さなモデルをトレーニングするよりも良いことがよくあります。

Phi-3.5-MoE-Instructモデルは、Phi-3.5-VisionやPhi-3.5-Instructよりも多くの計算能力を必要とします。Azure AI FoundryやNvidia NIMなどのクラウドベースの方法を使用して体験および使用することをお勧めします。

![nim2](../../../../../translated_images/nim2.ab50cc468e987efe5e87e8b9b2927f751b6d080c4a146129c2133da94b0f781e.ja.png)



### **🤖 Apple MLX向けPhi-3.5のサンプル**

| ラボ    | 紹介 | 行く |
| -------- | ------- |  ------- |
| 🚀 Lab-Introduce Phi-3.5 Instruct  | Phi-3.5 Instructの使い方を学ぶ |  [行く](../../../../../code/09.UpdateSamples/Aug/phi3-instruct-demo.ipynb)    |
| 🚀 Lab-Introduce Phi-3.5 Vision (image) | Phi-3.5 Visionを使用して画像を分析する方法を学ぶ |  [行く](../../../../../code/09.UpdateSamples/Aug/phi3-vision-demo.ipynb)    |
| 🚀 Lab-Introduce Phi-3.5 MoE   | Phi-3.5 Visionを使用して画像を分析する方法を学ぶ |  [行く](../../../../../code/09.UpdateSamples/Aug/phi3_moe_demo.ipynb)    |


## **リソース**

1. Hugging faceのPhi Family [https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)

2. GitHub Modelsについて [https://gh.io/models](https://gh.io/models)

3. Azure AI Foundryについて [https://ai.azure.com/](https://ai.azure.com/)

4. Nividia NIMについて [https://build.nvidia.com/explore/discover](https://build.nvidia.com/explore/discover)

**免責事項**:
この文書は機械ベースのAI翻訳サービスを使用して翻訳されています。正確さを期すために努力していますが、自動翻訳には誤りや不正確さが含まれる場合がありますのでご注意ください。元の言語で書かれた原文を権威ある情報源と見なすべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解について、当社は一切の責任を負いません。