# **Novedades de la Familia Phi-3.5**

¬øYa est√°s usando la Familia Phi-3? ¬øCu√°les son tus escenarios? El 20 de agosto de 2024, Microsoft lanz√≥ la nueva Familia Phi-3.5, que ha sido mejorada en aspectos multiling√ºes, de visi√≥n y en Agentes de IA. Hagamos una introducci√≥n m√°s detallada en conjunto con la Model Card en Hugging Face.

![PhiFamily](../../../../../translated_images/Phi3getstarted.086dfb90bb69325da6b717586337f2aec5decc241fda85e322eb55c709167f73.es.png)


## **Phi-3.5-mini-instruct**

Phi-3.5-mini es un modelo ligero y de √∫ltima generaci√≥n construido sobre los conjuntos de datos utilizados para Phi-3 - datos sint√©ticos y sitios web p√∫blicos filtrados - con un enfoque en datos de muy alta calidad y densos en razonamiento. El modelo pertenece a la familia de modelos Phi-3 y soporta una longitud de contexto de 128K tokens. El modelo pas√≥ por un riguroso proceso de mejora, incorporando tanto ajuste fino supervisado, optimizaci√≥n de pol√≠ticas proximales, como optimizaci√≥n de preferencias directas para asegurar una adherencia precisa a las instrucciones y medidas de seguridad robustas.

![benchmark1](../../../../../translated_images/benchmark1.479cb048e7d9239b09e562c410a54f6c9eaf85030af67ac6e7de80a69e4778a5.es.png)

![benchmark2](../../../../../translated_images/benchmark2.76982d411a07caa3ebd706dd6c0ba98b98a5609de371176a67cd619d70d4e6da.es.png)

A trav√©s de los indicadores en el Benchmark, puedes ver que Phi-3.5-mini ha mejorado el soporte para m√∫ltiples idiomas y contenido de texto largo en comparaci√≥n con Phi-3-mini, lo que se utiliza para mejorar las capacidades de lenguaje y texto de Phi-3.5 mini en aplicaciones de borde.

Podemos comparar la capacidad de conocimiento en chino a trav√©s de los Modelos de GitHub. Cuando preguntamos "¬øD√≥nde est√° Changsha?" (ÈïøÊ≤ôÂú®Âì™Èáå?), podemos comparar los resultados de Phi-3-mini-128k-instruct y Phi-3.5-mini-128k-instruct.

![Phi3](../../../../../translated_images/gh3.6b1a5c38ed732e40c0effaf4c558badfab0be6148b194aa6bec44adbfb1e4342.es.png)

![Phi35](../../../../../translated_images/gh35.b0fd2ff379a5f2d995ea1faedd2d7260cfcad7ffbad5a721a8a1b2b3d84028c8.es.png)

No es dif√≠cil ver que la mejora de datos en el corpus chino permite que Phi-3.5-mini tenga mejores resultados en escenarios b√°sicos de generaci√≥n de texto (***Nota:*** Tenga en cuenta que si Phi-3.5-mini necesita una respuesta m√°s precisa, se recomienda ajustarlo seg√∫n el escenario de aplicaci√≥n).

## **Phi-3.5-vision-instruct**

Phi-3.5-vision es un modelo multimodal ligero y de √∫ltima generaci√≥n construido sobre conjuntos de datos que incluyen - datos sint√©ticos y sitios web p√∫blicos filtrados - con un enfoque en datos de muy alta calidad y densos en razonamiento tanto en texto como en visi√≥n. El modelo pertenece a la familia de modelos Phi-3, y la versi√≥n multimodal viene con una longitud de contexto de 128K tokens que puede soportar. El modelo pas√≥ por un riguroso proceso de mejora, incorporando tanto ajuste fino supervisado como optimizaci√≥n de preferencias directas para asegurar una adherencia precisa a las instrucciones y medidas de seguridad robustas.

A trav√©s de Vision abrimos los ojos de la Familia Phi-3.x y pudimos completar los siguientes escenarios:

1. Entornos con restricciones de memoria/c√≥mputo
2. Escenarios con limitaciones de latencia
3. Comprensi√≥n general de im√°genes
4. Reconocimiento √≥ptico de caracteres
5. Comprensi√≥n de gr√°ficos y tablas
6. Comparaci√≥n de m√∫ltiples im√°genes
7. Resumen de m√∫ltiples im√°genes o clips de video

A trav√©s de Vision, permitimos que la Familia Phi abra sus ojos y complete los siguientes escenarios.

Tambi√©n podemos usar el benchmark proporcionado por Hugging Face para entender la comparaci√≥n en diferentes escenarios visuales.

![benchmark3](../../../../../translated_images/benchmark3.4d9484cc062f0c5076783f3cb33fe533c03995d3a5debc437420e88960032672.es.png)

Si deseas probar la versi√≥n gratuita de Phi-3.5-vision-instruct, podemos usar [Nivida NIM](https://build.nvidia.com/microsoft/phi-3_5-vision-instruct) para completar la experiencia.

![nim](../../../../../translated_images/nim.c985945596d6b2629658087485d16028a3874dcc37329de51b94adf09d0af661.es.png)

Por supuesto, tambi√©n puedes completar el despliegue a trav√©s de Azure AI Studio.

## **Phi-3.5-MoE-instruct**

Phi-3.5-MoE es un modelo ligero y de √∫ltima generaci√≥n construido sobre los conjuntos de datos utilizados para Phi-3 - datos sint√©ticos y documentos p√∫blicos filtrados - con un enfoque en datos de muy alta calidad y densos en razonamiento. El modelo soporta m√∫ltiples idiomas y viene con una longitud de contexto de 128K tokens. El modelo pas√≥ por un riguroso proceso de mejora, incorporando ajuste fino supervisado, optimizaci√≥n de pol√≠ticas proximales y optimizaci√≥n de preferencias directas para asegurar una adherencia precisa a las instrucciones y medidas de seguridad robustas.

Con el desarrollo de Agentes de IA, la demanda de modelos MoE aumentar√° gradualmente. MoE, cuyo nombre completo es Modelos de Expertos Mixtos, es un nuevo modelo formado por la mezcla de m√∫ltiples modelos expertos. MoE consiste en dividir primero el gran problema, luego resolver los peque√±os problemas uno por uno y finalmente resumir las conclusiones. Adem√°s, la escala del modelo es uno de los factores clave para mejorar el rendimiento del modelo. Con recursos de c√≥mputo limitados, entrenar un modelo m√°s grande con menos pasos de entrenamiento suele ser mejor que entrenar un modelo m√°s peque√±o con m√°s pasos.

El modelo Phi-3.5-MoE-Instruct requiere m√°s poder de c√≥mputo que Phi-3.5-Vision y Phi-3.5-Instruct. Se recomienda utilizar m√©todos basados en la nube como Azure AI Studio y Nvidia NIM para la experiencia y uso.

![nim2](../../../../../translated_images/nim2.ab50cc468e987efe5e87e8b9b2927f751b6d080c4a146129c2133da94b0f781e.es.png)



### **ü§ñ Ejemplos para Phi-3.5 con Apple MLX**

| Laboratorios | Introducci√≥n | Ir |
| ------------- | ------------ | -- |
| üöÄ Lab-Introduce Phi-3.5 Instruct  | Aprende a usar Phi-3.5 Instruct |  [Ir](../../../../../code/09.UpdateSamples/Aug/phi3-instruct-demo.ipynb)    |
| üöÄ Lab-Introduce Phi-3.5 Vision (imagen) | Aprende a usar Phi-3.5 Vision para analizar im√°genes |  [Ir](../../../../../code/09.UpdateSamples/Aug/phi3-vision-demo.ipynb)    |
| üöÄ Lab-Introduce Phi-3.5 MoE   | Aprende a usar Phi-3.5 Vision para analizar im√°genes |  [Ir](../../../../../code/09.UpdateSamples/Aug/phi3_moe_demo.ipynb)    |


## **Recursos**

1. Phi Family en Hugging Face [https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)

2. Sobre Modelos de GitHub [https://gh.io/models](https://gh.io/models)

3. Sobre Azure AI Studio [https://ai.azure.com/](https://ai.azure.com/)

4. Sobre Nvidia NIM [https://build.nvidia.com/explore/discover](https://build.nvidia.com/explore/discover)

Aviso legal: La traducci√≥n fue realizada a partir de su original por un modelo de IA y puede no ser perfecta. 
Por favor, revise el resultado y haga las correcciones necesarias.