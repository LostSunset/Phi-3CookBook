# 記載されている主要技術

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12上に構築されたハードウェアアクセラレーションによる機械学習のための低レベルAPI。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidiaによって開発された並列コンピューティングプラットフォームおよびAPIモデルで、GPUでの汎用処理を可能にします。
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - 機械学習モデルを表現するためのオープンフォーマットで、異なるMLフレームワーク間の相互運用性を提供します。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - 機械学習モデルの表現と更新に使用されるフォーマットで、特に4-8ビット量子化でCPU上で効果的に動作する小規模な言語モデルに有用です。

## DirectML

DirectMLは、ハードウェアアクセラレーションによる機械学習を可能にする低レベルAPIです。DirectX 12上に構築されており、GPUアクセラレーションを活用し、異なるGPUベンダー間でコード変更なしに動作するベンダー非依存の設計です。主にGPU上でのモデルトレーニングと推論に使用されます。

ハードウェアサポートに関しては、DirectMLはAMDの統合およびディスクリートGPU、Intelの統合GPU、NVIDIAのディスクリートGPUなど、幅広いGPUで動作するように設計されています。これはWindows AIプラットフォームの一部であり、Windows 10および11でサポートされているため、任意のWindowsデバイスでモデルのトレーニングと推論が可能です。

DirectMLに関連する更新と機会として、150のONNXオペレーターをサポートし、ONNXランタイムとWinMLの両方で使用されていることがあります。主要な統合ハードウェアベンダー（IHV）によってバックアップされており、それぞれが様々なメタコマンドを実装しています。

## CUDA

CUDA（Compute Unified Device Architecture）は、Nvidiaによって作成された並列コンピューティングプラットフォームおよびAPIモデルです。CUDA対応のGPUを汎用処理に使用することができ、これはGPGPU（Graphics Processing Unitsによる汎用計算）と呼ばれます。CUDAはNvidiaのGPUアクセラレーションの主要な推進力であり、機械学習、科学計算、ビデオ処理などのさまざまな分野で広く使用されています。

CUDAのハードウェアサポートはNvidiaのGPUに特化しており、各アーキテクチャは特定のバージョンのCUDAツールキットをサポートしています。これにより、開発者は必要なライブラリとツールを使用してCUDAアプリケーションを構築および実行できます。

## ONNX

ONNX（Open Neural Network Exchange）は、機械学習モデルを表現するためのオープンフォーマットです。拡張可能な計算グラフモデルの定義、組み込みオペレーターおよび標準データ型の定義を提供します。ONNXは開発者が異なるMLフレームワーク間でモデルを移動できるようにし、相互運用性を提供し、AIアプリケーションの作成とデプロイを容易にします。

Phi3 miniは、サーバープラットフォーム、Windows、Linux、Macデスクトップ、およびモバイルCPUを含むデバイス上で、CPUおよびGPUでONNXランタイムを使用して実行できます。追加された最適化構成は以下の通りです：

- int4 DML用のONNXモデル：AWQを介してint4に量子化
- fp16 CUDA用のONNXモデル
- int4 CUDA用のONNXモデル：RTNを介してint4に量子化
- int4 CPUおよびモバイル用のONNXモデル：RTNを介してint4に量子化

## Llama.cpp

Llama.cppは、C++で書かれたオープンソースのソフトウェアライブラリです。Llamaを含む様々な大型言語モデル（LLM）の推論を行います。ggmlライブラリ（汎用テンソルライブラリ）と共に開発され、元のPython実装と比較して高速な推論と低メモリ使用量を提供することを目指しています。ハードウェア最適化、量子化をサポートし、シンプルなAPIと例を提供します。効率的なLLM推論に興味があるなら、Phi3がLlama.cppを実行できるため、Llama.cppを探索する価値があります。

## GGUF

GGUF（Generic Graph Update Format）は、機械学習モデルの表現と更新に使用されるフォーマットです。特に4-8ビット量子化でCPU上で効果的に動作する小規模な言語モデル（SLM）に有用です。GGUFは迅速なプロトタイピングや、エッジデバイス上でのモデル実行、CI/CDパイプラインのバッチジョブなどに役立ちます。

**免責事項**：
この文書は、機械翻訳AIサービスを使用して翻訳されています。正確性を期していますが、自動翻訳には誤りや不正確さが含まれる場合があります。元の言語で書かれた文書が権威ある情報源と見なされるべきです。重要な情報については、専門の人間による翻訳を推奨します。この翻訳の使用に起因する誤解や誤解について、当社は一切の責任を負いません。