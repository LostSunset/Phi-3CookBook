# **Phi-3.5ファミリーの量子化**

モデルの量子化とは、ニューラルネットワークモデルのパラメータ（例えば重みや活性化値）を大きな値の範囲（通常は連続的な値の範囲）からより小さな有限の値の範囲にマッピングするプロセスを指します。この技術により、モデルのサイズと計算の複雑さを減少させ、モバイルデバイスや組み込みシステムなどのリソース制約のある環境でのモデルの動作効率を向上させることができます。モデルの量子化は、パラメータの精度を下げることで圧縮を実現しますが、一定の精度損失も伴います。したがって、量子化プロセスでは、モデルのサイズ、計算の複雑さ、精度のバランスを取る必要があります。一般的な量子化方法には、固定小数点量子化や浮動小数点量子化などがあります。具体的なシナリオやニーズに応じて適切な量子化戦略を選択できます。

私たちはGenAIモデルをエッジデバイスにデプロイし、モバイルデバイス、AI PC/Copilot+PC、従来のIoTデバイスなど、より多くのデバイスがGenAIシナリオに参加できるようにしたいと考えています。量子化モデルを通じて、異なるデバイスに基づいて異なるエッジデバイスにデプロイできます。ハードウェアメーカーが提供するモデル加速フレームワークと量子化モデルを組み合わせることで、より良いSLMアプリケーションシナリオを構築できます。

量子化シナリオでは、さまざまな精度（INT4、INT8、FP16、FP32）があります。以下に一般的な量子化精度の説明を示します。

### **INT4**

INT4量子化は、モデルの重みや活性化値を4ビットの整数に量子化する急進的な方法です。INT4量子化は、表現範囲が小さく精度が低いため、通常は精度の大きな損失をもたらします。しかし、INT8量子化と比較して、INT4量子化はモデルのストレージ要件と計算の複雑さをさらに減少させることができます。注意すべき点は、実際のアプリケーションではINT4量子化は比較的まれであり、精度が低すぎるとモデルのパフォーマンスが大幅に低下する可能性があるためです。さらに、すべてのハードウェアがINT4操作をサポートしているわけではないため、量子化方法を選択する際にはハードウェアの互換性を考慮する必要があります。

### **INT8**

INT8量子化は、モデルの重みや活性化値を浮動小数点数から8ビットの整数に変換するプロセスです。INT8整数が表現する数値範囲は小さく精度も低いですが、ストレージと計算要件を大幅に削減できます。INT8量子化では、モデルの重みや活性化値は量子化プロセスを経て、スケーリングとオフセットを含み、元の浮動小数点情報をできるだけ保持します。推論中に、これらの量子化された値は計算のために浮動小数点数にデクォンタイズされ、次のステップのために再びINT8に量子化されます。この方法は、ほとんどのアプリケーションで十分な精度を提供しながら、高い計算効率を維持できます。

### **FP16**

FP16フォーマット、つまり16ビット浮動小数点数（float16）は、32ビット浮動小数点数（float32）と比較してメモリ使用量を半分に減少させ、大規模なディープラーニングアプリケーションで顕著な利点があります。FP16フォーマットを使用すると、同じGPUメモリ制限内でより大きなモデルをロードしたり、より多くのデータを処理したりすることができます。最新のGPUハードウェアがFP16操作をサポートし続けるにつれて、FP16フォーマットを使用することで計算速度の向上も期待できます。しかし、FP16フォーマットには固有の欠点もあり、精度が低いため、数値の不安定性や一部のケースでの精度の損失を引き起こす可能性があります。

### **FP32**

FP32フォーマットは高い精度を提供し、広範な値を正確に表現できます。複雑な数学的操作が行われるシナリオや高精度の結果が必要な場合には、FP32フォーマットが好まれます。しかし、高精度はメモリ使用量の増加と計算時間の長さも意味します。大規模なディープラーニングモデルでは、特にモデルパラメータが多くデータ量が膨大な場合、FP32フォーマットはGPUメモリ不足や推論速度の低下を引き起こす可能性があります。

モバイルデバイスやIoTデバイスでは、Phi-3.xモデルをINT4に変換できますが、AI PC / Copilot PCではINT8、FP16、FP32などの高精度を使用できます。

現在、異なるハードウェアメーカーは異なるフレームワークを持ち、生成モデルをサポートしています。例えば、IntelのOpenVINO、QualcommのQNN、AppleのMLX、NvidiaのCUDAなどがあり、モデル量子化と組み合わせてローカルデプロイを完了します。

技術面では、量子化後に異なるフォーマットをサポートしています。例えば、PyTorch / Tensorflowフォーマット、GGUF、ONNXなどがあります。GGUFとONNXのフォーマット比較とアプリケーションシナリオを行いました。ここでは、モデルフレームワークからハードウェアまでのサポートが良好なONNX量子化フォーマットを推奨します。この章では、GenAIのためのONNX Runtime、OpenVINO、Apple MLXを使用してモデルの量子化を行います（より良い方法がある場合は、PRを提出してお知らせください）。

**この章には以下が含まれます**

1. [llama.cppを使用したPhi-3.5の量子化](./021.UsingLlamacppQuantifyingPhi35.md)

2. [onnxruntimeのGenerative AI拡張機能を使用したPhi-3.5の量子化](./022.UsingORTGenAIQuantifyingPhi35.md)

3. [Intel OpenVINOを使用したPhi-3.5の量子化](./023.UsingIntelOpenVINOQuantifyingPhi35.md)

4. [Apple MLXフレームワークを使用したPhi-3.5の量子化](./024.UsingAppleMLXQuantifyingPhi35.md)

免責事項: この翻訳はAIモデルによって原文から翻訳されたものであり、完璧ではない可能性があります。
出力を確認し、必要に応じて修正を行ってください。