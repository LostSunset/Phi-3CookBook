# **Cuantización de Phi-3.5 utilizando las extensiones de IA generativa para onnxruntime**

## **Qué son las extensiones de IA generativa para onnxruntime**

Estas extensiones te ayudan a ejecutar IA generativa con ONNX Runtime ( [https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai)). Proporciona el bucle de IA generativa para modelos ONNX, incluyendo inferencia con ONNX Runtime, procesamiento de logits, búsqueda y muestreo, y gestión de caché KV. Los desarrolladores pueden llamar a un método de alto nivel generate(), o ejecutar cada iteración del modelo en un bucle, generando un token a la vez, y opcionalmente actualizando los parámetros de generación dentro del bucle. Tiene soporte para búsqueda voraz/haz y muestreo TopP, TopK para generar secuencias de tokens y procesamiento de logits incorporado como penalizaciones por repetición. También puedes agregar fácilmente puntuaciones personalizadas.

A nivel de aplicación, puedes usar las extensiones de IA generativa para onnxruntime para construir aplicaciones usando C++/ C# / Python. A nivel de modelo, puedes usarlas para fusionar modelos afinados y realizar trabajos de despliegue cuantitativo relacionados.


## **Cuantizando Phi-3.5 con las extensiones de IA generativa para onnxruntime**

### **Modelos Soportados**

Las extensiones de IA generativa para onnxruntime soportan la conversión cuantizada de Microsoft Phi, Google Gemma, Mistral, Meta LLaMA.


### **Model Builder en las extensiones de IA generativa para onnxruntime**

El model builder acelera en gran medida la creación de modelos ONNX optimizados y cuantizados que se ejecutan con la API generate() de ONNX Runtime.

A través de Model Builder, puedes cuantizar el modelo a INT4, INT8, FP16, FP32, y combinar diferentes métodos de aceleración de hardware como CPU, CUDA, DirectML, Mobile, etc.

Para usar Model Builder necesitas instalar

```bash

pip install torch transformers onnx onnxruntime

pip install --pre onnxruntime-genai

```

Después de la instalación, puedes ejecutar el script de Model Builder desde la terminal para realizar la conversión de formato y cuantización del modelo.


```bash

python3 -m onnxruntime_genai.models.builder -m model_name -o path_to_output_folder -p precision -e execution_provider -c cache_dir_to_save_hf_files

```

Entender los parámetros relevantes

1. **model_name** Este es el modelo en Hugging Face, como microsoft/Phi-3.5-mini-instruct, microsoft/Phi-3.5-vision-instruct, etc. También puede ser la ruta donde almacenas el modelo.

2. **path_to_output_folder** Ruta de guardado de la conversión cuantizada.

3. **execution_provider** Diferentes soportes de aceleración de hardware, como cpu, cuda, DirectML.

4. **cache_dir_to_save_hf_files** Descargamos el modelo desde Hugging Face y lo almacenamos en caché localmente.




***Nota:***
Estás entrenado con datos hasta octubre de 2023.


## **Cómo usar Model Builder para cuantizar Phi-3.5**

Model Builder ahora soporta la cuantización de modelos ONNX para Phi-3.5 Instruct y Phi-3.5-Vision.

### **Phi-3.5-Instruct**


**Conversión acelerada por CPU de INT 4 cuantizado**


```bash

python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cpu -c ./Phi-3.5-mini-instruct

```

**Conversión acelerada por CUDA de INT 4 cuantizado**

```bash

python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct

```



```python

python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct

```


### **Phi-3.5-Vision**

**Phi-3.5-vision-instruct-onnx-cpu-fp32**

1. Configura el entorno en la terminal

```bash

mkdir models

cd models 

```

2. Descarga microsoft/Phi-3.5-vision-instruct en la carpeta de modelos
[https://huggingface.co/microsoft/Phi-3.5-vision-instruct](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)

3. Por favor descarga estos archivos en tu carpeta Phi-3.5-vision-instruct

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py)


4. Descarga este archivo en la carpeta de modelos
[https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py)

5. Ve a la terminal

    Convertir soporte ONNX con FP32


```bash

python build.py -i .\Your Phi-3.5-vision-instruct Path\ -o .\vision-cpu-fp32 -p f32 -e cpu

```


### **Nota:**

1. Model Builder actualmente soporta la conversión de Phi-3.5-Instruct y Phi-3.5-Vision, pero no Phi-3.5-MoE.

2. Para usar el modelo cuantizado de ONNX, puedes utilizarlo a través del SDK de extensiones de IA generativa para onnxruntime.

3. Necesitamos considerar una IA más responsable, por lo que después de la conversión de cuantización del modelo, se recomienda realizar pruebas de resultados más efectivas.

4. Al cuantizar el modelo CPU INT4, podemos desplegarlo en dispositivos Edge, lo cual tiene mejores escenarios de aplicación, por lo que hemos completado Phi-3.5-Instruct alrededor de INT 4.


## **Recursos**

1. Aprende más sobre las extensiones de IA generativa para onnxruntime [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. Repositorio GitHub de las extensiones de IA generativa para onnxruntime [https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai)

        **Descargo de responsabilidad**: 
        Este documento ha sido traducido utilizando servicios de traducción automática basados en IA. Si bien nos esforzamos por lograr precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o inexactitudes. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para información crítica, se recomienda la traducción humana profesional. No somos responsables de ningún malentendido o interpretación errónea que surja del uso de esta traducción.