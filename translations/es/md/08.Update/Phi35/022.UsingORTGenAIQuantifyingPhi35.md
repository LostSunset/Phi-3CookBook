# **Cuantificación de Phi-3.5 usando extensiones de IA generativa para onnxruntime**

## **Qué son las extensiones de IA generativa para onnxruntime**

Estas extensiones te ayudan a ejecutar IA generativa con ONNX Runtime ([https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai)). Proporciona el bucle de IA generativa para modelos ONNX, incluyendo inferencia con ONNX Runtime, procesamiento de logits, búsqueda y muestreo, y gestión de caché KV. Los desarrolladores pueden llamar al método de alto nivel generate(), o ejecutar cada iteración del modelo en un bucle, generando un token a la vez, y opcionalmente actualizando los parámetros de generación dentro del bucle. Tiene soporte para búsqueda voraz/beam search y muestreo TopP, TopK para generar secuencias de tokens y procesamiento de logits incorporado como penalizaciones por repetición. También puedes añadir fácilmente puntuaciones personalizadas.

A nivel de aplicación, puedes usar las extensiones de IA generativa para onnxruntime para construir aplicaciones usando C++/ C# / Python. A nivel de modelo, puedes usarlas para fusionar modelos afinados y realizar trabajos de despliegue cuantitativo relacionados.

## **Cuantificación de Phi-3.5 con extensiones de IA generativa para onnxruntime**

### **Modelos Soportados**

Las extensiones de IA generativa para onnxruntime soportan la conversión de cuantificación de Microsoft Phi, Google Gemma, Mistral, Meta LLaMA.

### **Constructor de Modelos en extensiones de IA generativa para onnxruntime**

El constructor de modelos acelera enormemente la creación de modelos ONNX optimizados y cuantificados que se ejecutan con la API generate() de ONNX Runtime.

A través del Constructor de Modelos, puedes cuantificar el modelo a INT4, INT8, FP16, FP32, y combinar diferentes métodos de aceleración de hardware como CPU, CUDA, DirectML, Mobile, etc.

Para usar el Constructor de Modelos necesitas instalar:

```bash

pip install torch transformers onnx onnxruntime

pip install --pre onnxruntime-genai

```

Después de la instalación, puedes ejecutar el script del Constructor de Modelos desde la terminal para realizar la conversión de formato y cuantificación del modelo.

```bash

python3 -m onnxruntime_genai.models.builder -m model_name -o path_to_output_folder -p precision -e execution_provider -c cache_dir_to_save_hf_files

```

Entiende los parámetros relevantes

1. **model_name** Este es el modelo en Hugging Face, como microsoft/Phi-3.5-mini-instruct, microsoft/Phi-3.5-vision-instruct, etc. También puede ser la ruta donde almacenas el modelo.

2. **path_to_output_folder** Ruta de guardado de la conversión cuantificada.

3. **execution_provider** Diferente soporte de aceleración de hardware, como cpu, cuda, DirectML.

4. **cache_dir_to_save_hf_files** Descargamos el modelo de Hugging Face y lo cacheamos localmente.

***Nota：***

## **Cómo usar el Constructor de Modelos para cuantificar Phi-3.5**

El Constructor de Modelos ahora soporta la cuantificación del modelo ONNX para Phi-3.5 Instruct y Phi-3.5-Vision.

### **Phi-3.5-Instruct**

**Conversión acelerada por CPU a INT 4 cuantificado**

```bash

python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct -o ./onnx-cpu -p int4 -e cpu -c ./Phi-3.5-mini-instruct

```

**Conversión acelerada por CUDA a INT 4 cuantificado**

```bash

python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct

```

```python

python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct

```

### **Phi-3.5-Vision**

**Phi-3.5-vision-instruct-onnx-cpu-fp32**

1. Configura el entorno en la terminal

```bash

mkdir models

cd models 

```

2. Descarga microsoft/Phi-3.5-vision-instruct en la carpeta models
[https://huggingface.co/microsoft/Phi-3.5-vision-instruct](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)

3. Por favor descarga estos archivos en tu carpeta Phi-3.5-vision-instruct

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py)

4. Descarga este archivo en la carpeta models
[https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py)

5. Ve a la terminal

   Convierte a soporte ONNX con FP32

```bash

python build.py -i .\Your Phi-3.5-vision-instruct Path\ -o .\vision-cpu-fp32 -p f32 -e cpu

```

### **Nota：**

1. El Constructor de Modelos actualmente soporta la conversión de Phi-3.5-Instruct y Phi-3.5-Vision, pero no de Phi-3.5-MoE.

2. Para usar el modelo cuantificado de ONNX, puedes usarlo a través del SDK de extensiones de IA generativa para onnxruntime.

3. Necesitamos considerar una IA más responsable, así que después de la conversión de cuantificación del modelo, se recomienda realizar pruebas de resultados más efectivas.

4. Al cuantificar el modelo CPU INT4, podemos desplegarlo en dispositivos Edge, lo que tiene mejores escenarios de aplicación. Por eso hemos completado Phi-3.5-Instruct en torno a INT 4.

## **Recursos**

1. Aprende más sobre las extensiones de IA generativa para onnxruntime [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. Repositorio de GitHub de extensiones de IA generativa para onnxruntime [https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai)

        Descargo de responsabilidad: La traducción fue realizada por un modelo de IA y puede no ser perfecta.
        Por favor, revise el resultado y haga las correcciones necesarias.