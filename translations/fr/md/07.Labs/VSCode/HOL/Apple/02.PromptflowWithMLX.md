# **Lab 2 - Exécuter Prompt flow avec Phi-3-mini dans AIPC**

## **Qu'est-ce que Prompt flow**

Prompt flow est une suite d'outils de développement conçue pour simplifier le cycle de développement complet des applications IA basées sur les LLM, de l'idéation, la création de prototypes, les tests, l'évaluation au déploiement en production et à la surveillance. Il facilite grandement l'ingénierie des prompts et vous permet de créer des applications LLM de qualité production.

Avec Prompt flow, vous pourrez :

- Créer des flux qui lient les LLM, les prompts, le code Python et d'autres outils ensemble dans un flux de travail exécutable.

- Déboguer et itérer vos flux, en particulier l'interaction avec les LLM, en toute simplicité.

- Évaluer vos flux, calculer des métriques de qualité et de performance avec des ensembles de données plus grands.

- Intégrer les tests et l'évaluation dans votre système CI/CD pour garantir la qualité de votre flux.

- Déployer vos flux sur la plateforme de service de votre choix ou les intégrer facilement dans le code de votre application.

- (Optionnel mais fortement recommandé) Collaborer avec votre équipe en utilisant la version cloud de Prompt flow dans Azure AI.



## **Construire des flux de génération de code sur Apple Silicon**

***Note*** : Si vous n'avez pas terminé l'installation de l'environnement, veuillez visiter [Lab 0 -Installations](./01.Installations.md)

1. Ouvrez l'extension Prompt flow dans Visual Studio Code et créez un projet de flux vide

![create](../../../../../../../translated_images/pf_create.626fd367cf0ac7981e0731fdfc70fa46df0826f9eaf57c22f07908817ede14d3.fr.png)

2. Ajoutez des paramètres d'entrées et de sorties et ajoutez du code Python comme nouveau flux

![flow](../../../../../../../translated_images/pf_flow.f2d64298a737b204ec7b33604538c97d4fffe9e07e74bad1c162e88e026d3dfa.fr.png)

Vous pouvez vous référer à cette structure (flow.dag.yaml) pour construire votre flux

```yaml

inputs:
  prompt:
    type: string
    default: Write python code for Fibonacci serie. Please use markdown as output
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
- name: gen_code_by_phi3
  type: python
  source:
    type: code
    path: gen_code_by_phi3.py
  inputs:
    prompt: ${inputs.prompt}


```

3. Quantifier phi-3-mini

Nous espérons mieux exécuter SLM sur des appareils locaux. En général, nous quantifions le modèle (INT4, FP16, FP32)


```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

**Note:** le dossier par défaut est mlx_model 

4. Ajoutez du code dans ***Chat_With_Phi3.py***


```python


from promptflow import tool

from mlx_lm import load, generate


# La section inputs changera en fonction des arguments de la fonction tool, après avoir enregistré le code
# Ajouter un type aux arguments et à la valeur de retour aidera le système à afficher correctement les types
# Veuillez mettre à jour le nom/la signature de la fonction selon les besoins
@tool
def my_python_tool(prompt: str) -> str:

    model_id = './mlx_model_phi3_mini'

    model, tokenizer = load(model_id)

    # <|user|>\nWrite python code for Fibonacci serie. Please use markdown as output<|end|>\n<|assistant|>

    response = generate(model, tokenizer, prompt="<|user|>\n" + prompt  + "<|end|>\n<|assistant|>", max_tokens=2048, verbose=True)

    return response


```

4. Vous pouvez tester le flux en mode Debug ou Run pour vérifier si la génération de code fonctionne correctement 

![RUN](../../../../../../../translated_images/pf_run.57c3f9e7e7052ff85850b8f06648c7d5b4d2ac9f4796381fd8d29b1a41e1f705.fr.png)

5. Exécutez le flux en tant qu'API de développement dans le terminal

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Vous pouvez le tester dans Postman / Thunder Client


### **Note**

1. La première exécution prend beaucoup de temps. Il est recommandé de télécharger le modèle phi-3 depuis Hugging face CLI.

2. En raison de la puissance de calcul limitée du NPU Intel, il est recommandé d'utiliser Phi-3-mini-4k-instruct

3. Nous utilisons l'accélération NPU Intel pour quantifier la conversion INT4, mais si vous relancez le service, vous devez supprimer les dossiers cache et nc_workshop.



## **Ressources**

1. Apprendre Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Apprendre l'accélération NPU Intel [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Code d'exemple, télécharger [Local NPU Agent Sample Code](../../../../../code/07.Lab/01/AIPC/local-npu-agent/)

Avertissement : La traduction a été réalisée à partir de l'original par un modèle d'IA et peut ne pas être parfaite. 
Veuillez examiner le résultat et apporter les corrections nécessaires.