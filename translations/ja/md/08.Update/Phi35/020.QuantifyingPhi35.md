# **Phi-3.5ファミリーの量子化**

モデルの量子化とは、ニューラルネットワークモデルのパラメータ（重みや活性化値など）を、大きな値の範囲（通常は連続した値の範囲）から、より小さな有限の値の範囲にマッピングするプロセスを指します。この技術は、モデルのサイズと計算の複雑さを減らし、モバイルデバイスや組み込みシステムなどのリソースが制約された環境でのモデルの運用効率を向上させることができます。モデルの量子化は、パラメータの精度を下げることで圧縮を実現しますが、同時にある程度の精度の損失も引き起こします。したがって、量子化のプロセスでは、モデルのサイズ、計算の複雑さ、および精度のバランスを取る必要があります。一般的な量子化方法には、固定小数点量子化、浮動小数点量子化などがあります。具体的なシナリオやニーズに応じて、適切な量子化戦略を選択できます。

私たちは、GenAIモデルをエッジデバイスにデプロイし、モバイルデバイス、AI PC/Copilot+PC、従来のIoTデバイスなど、より多くのデバイスがGenAIシナリオに参加できるようにしたいと考えています。量子化モデルを通じて、異なるデバイスに基づいて異なるエッジデバイスにデプロイできます。ハードウェアメーカーが提供するモデル加速フレームワークと量子化モデルを組み合わせることで、より良いSLMアプリケーションシナリオを構築できます。

量子化シナリオでは、異なる精度（INT4、INT8、FP16、FP32）があります。以下に、一般的に使用される量子化精度について説明します。

### **INT4**

INT4量子化は、モデルの重みと活性化値を4ビットの整数に量子化する過激な量子化方法です。INT4量子化は、表現範囲が小さく精度が低いため、通常は精度の損失が大きくなります。しかし、INT8量子化と比較して、INT4量子化はモデルのストレージ要件と計算の複雑さをさらに減らすことができます。ただし、INT4量子化は実際のアプリケーションでは比較的まれであり、精度が低すぎるとモデルのパフォーマンスが著しく低下する可能性があるため注意が必要です。さらに、すべてのハードウェアがINT4操作をサポートしているわけではないため、量子化方法を選択する際にはハードウェアの互換性を考慮する必要があります。

### **INT8**

INT8量子化は、モデルの重みと活性化値を浮動小数点数から8ビットの整数に変換するプロセスです。INT8整数が表現する数値範囲は小さく精度も低いですが、ストレージと計算要件を大幅に削減できます。INT8量子化では、モデルの重みと活性化値がスケーリングとオフセットを含む量子化プロセスを経て、元の浮動小数点情報をできるだけ保持します。推論中に、これらの量子化された値は計算のために再び浮動小数点数にデクアンタイズされ、その後次のステップのために再びINT8に量子化されます。この方法は、多くのアプリケーションで十分な精度を提供しながら、高い計算効率を維持できます。

### **FP16**

FP16フォーマット、つまり16ビット浮動小数点数（float16）は、32ビット浮動小数点数（float32）と比較してメモリの使用量を半分に減らし、大規模な深層学習アプリケーションで大きな利点があります。FP16フォーマットは、同じGPUメモリ制限内でより大きなモデルをロードしたり、より多くのデータを処理したりすることを可能にします。現代のGPUハードウェアがFP16操作をサポートし続ける中で、FP16フォーマットを使用することで計算速度の向上も期待できます。ただし、FP16フォーマットには固有の欠点もあり、精度が低いため、場合によっては数値の不安定性や精度の損失が生じる可能性があります。

### **FP32**

FP32フォーマットはより高い精度を提供し、広範囲の値を正確に表現できます。複雑な数学的操作が行われるシナリオや高精度の結果が必要なシナリオでは、FP32フォーマットが優先されます。ただし、高精度はより多くのメモリ使用量と長い計算時間も意味します。大規模な深層学習モデルでは、特にモデルパラメータが多くデータ量が膨大な場合、FP32フォーマットはGPUメモリの不足や推論速度の低下を引き起こす可能性があります。

モバイルデバイスやIoTデバイスでは、Phi-3.xモデルをINT4に変換できますが、AI PC / Copilot PCではINT8、FP16、FP32などの高精度を使用できます。

現在、異なるハードウェアメーカーは、IntelのOpenVINO、QualcommのQNN、AppleのMLX、NvidiaのCUDAなど、生成モデルをサポートする異なるフレームワークを持っており、モデル量子化と組み合わせてローカルデプロイを完了します。

技術的には、量子化後に異なるフォーマットをサポートしています。例えば、PyTorch / Tensorflowフォーマット、GGUF、ONNXなどです。GGUFとONNXのフォーマット比較と適用シナリオを行いました。ここでは、モデルフレームワークからハードウェアまでのサポートが良好なONNX量子化フォーマットをお勧めします。この章では、GenAIのためのONNX Runtime、OpenVINO、およびApple MLXを使用してモデル量子化を行います（より良い方法があれば、PRを提出していただければ幸いです）。

**この章に含まれる内容**

1. [llama.cppを使用したPhi-3.5の量子化](./021.UsingLlamacppQuantifyingPhi35.md)

2. [onnxruntimeの生成AI拡張機能を使用したPhi-3.5の量子化](./022.UsingORTGenAIQuantifyingPhi35.md)

3. [Intel OpenVINOを使用したPhi-3.5の量子化](./023.UsingIntelOpenVINOQuantifyingPhi35.md)

4. [Apple MLXフレームワークを使用したPhi-3.5の量子化](./024.UsingAppleMLXQuantifyingPhi35.md)

**免責事項**:
この文書は機械翻訳AIサービスを使用して翻訳されています。正確さを期すよう努めておりますが、自動翻訳には誤りや不正確さが含まれる可能性があることをご了承ください。元の言語での原文が権威ある情報源と見なされるべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用に起因する誤解や誤訳について、当社は責任を負いかねます。