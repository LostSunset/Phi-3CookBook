{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Whisperを使用したインタラクティブPhi 3 Mini 4K Instructチャットボット\n",
        "\n",
        "### イントロダクション:\n",
        "インタラクティブPhi 3 Mini 4K Instructチャットボットは、ユーザーがテキストまたは音声入力を使用してMicrosoft Phi 3 Mini 4K Instructデモと対話できるツールです。このチャットボットは、翻訳、天気の更新、一般的な情報収集など、さまざまなタスクに使用できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Atl_WEmtR0Yd"
      },
      "outputs": [],
      "source": [
        "# 必要なPythonパッケージをインストール\n",
        "!pip install accelerate\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install flash-attn --no-build-isolation', env={'FLASH_ATTENTION_SKIP_CUDA_BUILD': \"TRUE\"}, shell=True\n",
        "!pip install transformers\n",
        "!pip install wheel\n",
        "!pip install gradio\n",
        "!pip install pydub==0.25.1\n",
        "!pip install edge-tts\n",
        "!pip install openai-whisper==20231117\n",
        "!pip install ffmpeg==1.4\n",
        "# from IPython.display import clear_output\n",
        "# clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cudaサポートが利用可能かどうかを確認\n",
        "# 出力がTrueの場合 = Cuda\n",
        "# 出力がFalseの場合 = Cudaなし（モデルをGPUで実行するにはCudaのインストールが必要）\n",
        "import os \n",
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKAUp20H4ZXl"
      },
      "source": [
        "[Huggingfaceアクセス トークンを作成](https://huggingface.co/settings/tokens)\n",
        "\n",
        "新しいトークンを作成 \n",
        "新しい名前を提供 \n",
        "書き込み権限を選択\n",
        "トークンをコピーして安全な場所に保存\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "次のPythonコードは、2つの主要なタスクを実行します：`os`モジュールのインポートと環境変数の設定。\n",
        "\n",
        "1. `os`モジュールのインポート：\n",
        "   - Pythonの`os`モジュールは、オペレーティングシステムと対話する方法を提供します。環境変数へのアクセス、ファイルやディレクトリの操作など、さまざまなオペレーティングシステム関連のタスクを実行できます。\n",
        "   - このコードでは、`import`文を使用して`os`モジュールをインポートしています。この文により、`os`モジュールの機能が現在のPythonスクリプトで使用可能になります。\n",
        "\n",
        "2. 環境変数の設定：\n",
        "   - 環境変数は、オペレーティングシステム上で実行されるプログラムがアクセスできる値です。これは、複数のプログラムで使用できる設定やその他の情報を保存する方法です。\n",
        "   - このコードでは、`os.environ`ディクショナリを使用して新しい環境変数を設定しています。ディクショナリのキーは`'HF_TOKEN'`で、値は`HUGGINGFACE_TOKEN`変数から取得されます。\n",
        "   - `HUGGINGFACE_TOKEN`変数は、このコードスニペットの上に定義されており、`#@param`構文を使用して文字列値`\"hf_**************\"`に割り当てられています。この構文は、Jupyterノートブックでよく使用され、ノートブックインターフェイスでユーザー入力とパラメータ設定を直接行うことができます。\n",
        "   - `'HF_TOKEN'`環境変数を設定することで、プログラムの他の部分や同じオペレーティングシステム上で実行される他のプログラムがアクセスできるようになります。\n",
        "\n",
        "全体として、このコードは`os`モジュールをインポートし、`HUGGINGFACE_TOKEN`変数に提供された値を使用して`'HF_TOKEN'`という名前の環境変数を設定します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N5r2ikbwR68c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Hugging Faceトークンを設定\n",
        "# Hugging Faceトークンを環境変数に追加\n",
        "HUGGINGFACE_TOKEN = \"Enter Hugging Face Key\" #@param {type:\"string\"}\n",
        "os.environ['HF_TOKEN'] = HUGGINGFACE_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nmXm0dxuRinA"
      },
      "outputs": [],
      "source": [
        "# Phi-3-mini-4k-instructモデルとWhisper Tinyをダウンロード\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "# 音声からテキストへの変換用のwhisper\n",
        "import whisper\n",
        "select_model =\"tiny\" # ['tiny', 'base']\n",
        "whisper_model = whisper.load_model(select_model)\n",
        "\n",
        "#from IPython.display import clear_output\n",
        "#clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Edge TTSサービスを使用してテキストを音声に変換（TTS）します。以下の関数実装を一つずつ見ていきましょう：\n",
        "\n",
        "1. `calculate_rate_string(input_value)`：この関数は入力値を受け取り、TTS音声の速度文字列を計算します。入力値は音声の速度を表し、値1は通常の速度を表します。関数は入力値から1を引き、それを100倍し、入力値が1以上かどうかに基づいて符号を決定します。関数は速度文字列を\"{sign}{rate}\"の形式で返します。\n",
        "\n",
        "2. `make_chunks(input_text, language)`：この関数は入力テキストと言語をパラメータとして受け取ります。言語固有のルールに基づいて入力テキストをチャンクに分割します。この実装では、言語が\"English\"の場合、関数は各ピリオド（\".\"）でテキストを分割し、先頭または末尾の空白を削除します。次に、各チャンクにピリオドを追加し、フィルタリングされたチャンクのリストを返します。\n",
        "\n",
        "3. `tts_file_name(text)`：この関数は入力テキストに基づいてTTS音声ファイルのファイル名を生成します。テキストに対していくつかの変換を行います：末尾のピリオドを削除（存在する場合）、テキストを小文字に変換、先頭と末尾の空白を削除し、スペースをアンダースコアに置き換えます。次に、テキストを最大25文字に切り詰め（長い場合）、または空の場合は完全なテキストを使用します。最後に、[`uuid`]モジュールを使用してランダムな文字列を生成し、それを切り詰めたテキストと組み合わせてファイル名を作成します。形式は\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"です。\n",
        "\n",
        "4. `merge_audio_files(audio_paths, output_path)`：この関数は複数の音声ファイルを1つの音声ファイルに結合します。音声ファイルのパスのリストと出力パスをパラメータとして受け取ります。関数は空の`AudioSegment`オブジェクトを初期化し、[`merged_audio`]と呼びます。次に、各音声ファイルのパスを反復処理し、`pydub`ライブラリの`AudioSegment.from_file()`メソッドを使用して音声ファイルをロードし、現在の音声ファイルを[`merged_audio`]オブジェクトに追加します。最後に、結合された音声を指定された出力パスにMP3形式でエクスポートします。\n",
        "\n",
        "5. `edge_free_tts(chunks_list, speed, voice_name, save_path)`：この関数はEdge TTSサービスを使用してTTS操作を実行します。テキストチャンクのリスト、音声の速度、音声名、および保存パスをパラメータとして受け取ります。チャンクの数が1より大きい場合、関数は個々のチャンクの音声ファイルを保存するためのディレクトリを作成します。次に、各チャンクを反復処理し、`calculate_rate_string()`関数、音声名、およびチャンクテキストを使用してEdge TTSコマンドを構築し、`os.system()`関数を使用してコマンドを実行します。コマンドの実行が成功した場合、生成された音声ファイルのパスをリストに追加します。すべてのチャンクを処理した後、`merge_audio_files()`関数を使用して個々の音声ファイルを結合し、結合された音声を指定された保存パスに保存します。チャンクが1つだけの場合、Edge TTSコマンドを直接生成し、音声を保存パスに保存します。最後に、生成された音声ファイルの保存パスを返します。\n",
        "\n",
        "6. `random_audio_name_generate()`：この関数は[`uuid`]モジュールを使用してランダムな音声ファイル名を生成します。ランダムなUUIDを生成し、それを文字列に変換し、最初の8文字を取り、\".mp3\"拡張子を追加してランダムな音声ファイル名を返します。\n",
        "\n",
        "7. `talk(input_text)`：この関数はTTS操作を実行するための主要なエントリポイントです。入力テキストをパラメータとして受け取ります。まず、入力テキストの長さを確認して、それが長い文（600文字以上）かどうかを判断します。長さと`translate_text_flag`変数の値に基づいて、言語を決定し、`make_chunks()`関数を使用してテキストチャンクのリストを生成します。次に、`random_audio_name_generate()`関数を使用して音声ファイルの保存パスを生成します。最後に、`edge_free_tts()`関数を呼び出してTTS操作を実行し、生成された音声ファイルの保存パスを返します。\n",
        "\n",
        "全体として、これらの関数は協力して入力テキストをチャンクに分割し、音声ファイルのファイル名を生成し、Edge TTSサービスを使用してTTS操作を実行し、個々の音声ファイルを1つの音声ファイルに結合します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "Mv4WVhNUz4IL",
        "outputId": "7f177f73-3eb1-4d7c-d5e9-1e7cabe32f63"
      },
      "outputs": [],
      "source": [
        "#@title Edge TTS\n",
        "def calculate_rate_string(input_value):\n",
        "    rate = (input_value - 1) * 100\n",
        "    sign = '+' if input_value >= 1 else '-'\n",
        "    return f\"{sign}{abs(int(rate))}\"\n",
        "\n",
        "\n",
        "def make_chunks(input_text, language):\n",
        "    language=\"English\"\n",
        "    if language == \"English\":\n",
        "      temp_list = input_text.strip().split(\".\")\n",
        "      filtered_list = [element.strip() + '.' for element in temp_list[:-1] if element.strip() and element.strip() != \"'\" and element.strip() != '\"']\n",
        "      if temp_list[-1].strip():\n",
        "          filtered_list.append(temp_list[-1].strip())\n",
        "      return filtered_list\n",
        "\n",
        "\n",
        "import re\n",
        "import uuid\n",
        "def tts_file_name(text):\n",
        "    if text.endswith(\".\"):\n",
        "        text = text[:-1]\n",
        "    text = text.lower()\n",
        "    text = text.strip()\n",
        "    text = text.replace(\" \",\"_\")\n",
        "    truncated_text = text[:25] if len(text) > 25 else text if len(text) > 0 else \"empty\"\n",
        "    random_string = uuid.uuid4().hex[:8].upper()\n",
        "    file_name = f\"/content/edge_tts_voice/{truncated_text}_{random_string}.mp3\"\n",
        "    return file_name\n",
        "\n",
        "\n",
        "from pydub import AudioSegment\n",
        "import shutil\n",
        "import os\n",
        "def merge_audio_files(audio_paths, output_path):\n",
        "    # 空のAudioSegmentを初期化\n",
        "    merged_audio = AudioSegment.silent(duration=0)\n",
        "\n",
        "    # 各音声ファイルのパスを反復処理\n",
        "    for audio_path in audio_paths:\n",
        "        # Pydubを使用して音声ファイルをロード\n",
        "        audio = AudioSegment.from_file(audio_path)\n",
        "\n",
        "        # 現在の音声ファイルをmerged_audioに追加\n",
        "        merged_audio += audio\n",
        "\n",
        "    # 結合された音声を指定された出力パスにエクスポート\n",
        "    merged_audio.export(output_path, format=\"mp3\")\n",
        "\n",
        "def edge_free_tts(chunks_list,speed,voice_name,save_path):\n",
        "  # print(chunks_list)\n",
        "  if len(chunks_list)>1:\n",
        "    chunk_audio_list=[]\n",
        "    if os.path.exists(\"/content/edge_tts_voice\"):\n",
        "      shutil.rmtree(\"/content/edge_tts_voice\")\n",
        "    os.mkdir(\"/content/edge_tts_voice\")\n",
        "    k=1\n",
        "    for i in chunks_list:\n",
        "      print(i)\n",
        "      edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{i}\" --write-media /content/edge_tts_voice/{k}.mp3'\n",
        "      print(edge_command)\n",
        "      var1=os.system(edge_command)\n",
        "      if var1==0:\n",
        "        pass\n",
        "      else:\n",
        "        print(f\"Failed: {i}\")\n",
        "      chunk_audio_list.append(f\"/content/edge_tts_voice/{k}.mp3\")\n",
        "      k+=1\n",
        "    # print(chunk_audio_list)\n",
        "    merge_audio_files(chunk_audio_list, save_path)\n",
        "  else:\n",
        "    edge_command=f'edge-tts  --rate={calculate_rate_string(speed)}% --voice {voice_name} --text \"{chunks_list[0]}\" --write-media {save_path}'\n",
        "    print(edge_command)\n",
        "    var2=os.system(edge_command)\n",
        "    if var2==0:\n",
        "      pass\n",
        "    else:\n",
        "      print(f\"Failed: {chunks_list[0]}\")\n",
        "  return save_path\n",
        "\n",
        "# text = \"This is Microsoft Phi 3 mini 4k instruct Demo\" Simply update the text variable with the text you want to convert to speech\n",
        "text = 'This is Microsoft Phi 3 mini 4k instruct Demo'  # @param {type: \"string\"}\n",
        "Language = \"English\" # @param ['English']\n",
        "# 音声の性別を男性から女性に変更し、使用したい音声を選択\n",
        "Gender = \"Female\"# @param ['Male', 'Female']\n",
        "female_voice=\"en-US-AriaNeural\"# @param[\"en-US-AriaNeural\",'zh-CN-XiaoxiaoNeural','zh-CN-XiaoyiNeural']\n",
        "speed = 1  # @param {type: \"number\"}\n",
        "translate_text_flag  = False\n",
        "if len(text)>=600:\n",
        "  long_sentence = True\n",
        "else:\n",
        "  long_sentence = False\n",
        "\n",
        "# long_sentence = False # @param {type:\"boolean\"}\n",
        "save_path = ''  # @param {type: \"string\"}\n",
        "if len(save_path)==0:\n",
        "  save_path=tts_file_name(text)\n",
        "if Language == \"English\" :\n",
        "  if Gender==\"Male\":\n",
        "    voice_name=\"en-US-ChristopherNeural\"\n",
        "  if Gender==\"Female\":\n",
        "    voice_name=female_voice\n",
        "    # voice_name=\"en-US-AriaNeural\"\n",
        "\n",
        "\n",
        "if translate_text_flag:\n",
        "  input_text=text\n",
        "  # input_text=translate_text(text, Language)\n",
        "  # print(\"Translateting\")\n",
        "else:\n",
        "  input_text=text\n",
        "if long_sentence==True and translate_text_flag==True:\n",
        "  chunks_list=make_chunks(input_text,Language)\n",
        "elif long_sentence==True and translate_text_flag==False:\n",
        "  chunks_list=make_chunks(input_text,\"English\")\n",
        "else:\n",
        "  chunks_list=[input_text]\n",
        "# print(chunks_list)\n",
        "# edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
        "# from IPython.display import clear_output\n",
        "# clear_output()\n",
        "# from IPython.display import Audio\n",
        "# Audio(edge_save_path, autoplay=True)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import Audio\n",
        "if not os.path.exists(\"/content/audio\"):\n",
        "    os.mkdir(\"/content/audio\")\n",
        "import uuid\n",
        "def random_audio_name_generate():\n",
        "  random_uuid = uuid.uuid4()\n",
        "  audio_extension = \".mp3\"\n",
        "  random_audio_name = str(random_uuid)[:8] + audio_extension\n",
        "  return random_audio_name\n",
        "def talk(input_text):\n",
        "  global translate_text_flag,Language,speed,voice_name\n",
        "  if len(input_text)>=600:\n",
        "    long_sentence = True\n",
        "  else:\n",
        "    long_sentence = False\n",
        "\n",
        "  if long_sentence==True and translate_text_flag==True:\n",
        "    chunks_list=make_chunks(input_text,Language)\n",
        "  elif long_sentence==True and translate_text_flag==False:\n",
        "    chunks_list=make_chunks(input_text,\"English\")\n",
        "  else:\n",
        "    chunks_list=[input_text]\n",
        "  save_path=\"/content/audio/\"+random_audio_name_generate()\n",
        "  edge_save_path=edge_free_tts(chunks_list,speed,voice_name,save_path)\n",
        "  return edge_save_path\n",
        "\n",
        "\n",
        "edge_save_path=talk(text)\n",
        "Audio(edge_save_path, autoplay=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2つの関数`convert_to_text`と`run_text_prompt`の実装、および2つのクラス`str`と`Audio`の宣言。\n",
        "\n",
        "関数`convert_to_text`は、`audio_path`を入力として受け取り、`whisper_model`というモデルを使用して音声をテキストに変換します。関数はまず、`gpu`フラグが`True`に設定されているかどうかを確認します。もしそうであれば、`whisper_model`を使用し、`word_timestamps=True`、`fp16=True`、`language='English'`、`task='translate'`などのパラメータを設定します。`gpu`フラグが`False`の場合、`fp16=False`を使用して`whisper_model`を使用します。生成された転写結果は`scan.txt`というファイルに保存され、テキストとして返されます。\n",
        "\n",
        "関数`run_text_prompt`は、メッセージと`chat_history`を入力として受け取ります。`phi_demo`関数を使用して、入力メッセージに基づいてチャットボットからの応答を生成します。生成された応答は`talk`関数に渡され、応答が音声ファイルに変換され、ファイルパスが返されます。`Audio`クラスは音声ファイルを表示および再生するために使用されます。音声は`IPython.display`モジュールの`display`関数を使用して表示され、`autoplay=True`パラメータを使用して`Audio`オブジェクトが作成されるため、音声は自動的に再生されます。`chat_history`は入力メッセージと生成された応答で更新され、空の文字列と更新された`chat_history`が返されます。\n",
        "\n",
        "`str`クラスは、文字のシーケンスを表すPythonの組み込みクラスです。文字列の操作や処理のためのさまざまなメソッドを提供します。これらのメソッドには、`capitalize`、`casefold`、`center`、`count`、`encode`、`endswith`、`expandtabs`、`find`、`format`、`index`、`isalnum`、`isalpha`、`isascii`、`isdecimal`、`isdigit`、`isidentifier`、`islower`、`isnumeric`、`isprintable`、`isspace`、`istitle`、`isupper`、`join`、`ljust`、`lower`、`lstrip`、`partition`、`replace`、`removeprefix`、`removesuffix`、`rfind`、`rindex`、`rjust`、`rpartition`、`rsplit`、`rstrip`、`split`、`splitlines`、`startswith`、`strip`、`swapcase`、`title`、`translate`、`upper`、`zfill`などがあります。これらのメソッドを使用して、文字列の検索、置換、フォーマット、および操作を行うことができます。\n",
        "\n",
        "`Audio`クラスは、音声オブジェクトを表すカスタムクラスです。Jupyter Notebook環境で音声プレーヤーを作成するために使用されます。このクラスは、`data`、`filename`、`url`、`embed`、`rate`、`autoplay`、および`normalize`などのさまざまなパラメータを受け入れます。`data`パラメータは、numpy配列、サンプルのリスト、ファイル名またはURLを表す文字列、または生のPCMデータである可能性があります。`filename`パラメータは、音声データをロードするローカルファイルを指定するために使用され、`url`パラメータは、音声データをダウンロードするURLを指定するために使用されます。`embed`パラメータは、音声データをデータURIを使用して埋め込むか、元のソースから参照するかを決定します。`rate`パラメータは、音声データのサンプリングレートを指定します。`autoplay`パラメータは、音声が自動的に再生を開始するかどうかを決定します。`normalize`パラメータは、音声データが最大可能範囲に正規化（再スケーリング）されるかどうかを指定します。`Audio`クラスは、ファイルまたはURLから音声データを再ロードするための`reload`などのメソッドや、HTMLの音声要素の対応する属性を取得するための`src_attr`、`autoplay_attr`、および`element_id_attr`などの属性も提供します。\n",
        "\n",
        "全体として、これらの関数とクラスは、音声をテキストに転写し、チャットボットからの音声応答を生成し、Jupyter Notebook環境で音声を表示および再生するために使用されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e6aTA6mk7Gi",
        "outputId": "4c4825c9-f1ef-4d9e-d294-83d67248e073"
      },
      "outputs": [],
      "source": [
        "#@title Run gradio app\n",
        "def convert_to_text(audio_path):\n",
        "  gpu=True\n",
        "  if gpu:\n",
        "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=True,language='English',task='translate')\n",
        "  else:\n",
        "    result = whisper_model.transcribe(audio_path,word_timestamps=True,fp16=False,language='English',task='translate')\n",
        "  with open('scan.txt', 'w') as file:\n",
        "    file.write(str(result))\n",
        "  return result[\"text\"]\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "from IPython.display import Audio, display\n",
        "def run_text_prompt(message, chat_history):\n",
        "    bot_message = phi_demo(message)\n",
        "    edge_save_path=talk(bot_message)\n",
        "    # print(edge_save_path)\n",
        "    display(Audio(edge_save_path, autoplay=True))\n",
        "\n",
        "    chat_history.append((message, bot_message))\n",
        "    return \"\", chat_history\n",
        "\n",
        "\n",
        "def run_audio_prompt(audio, chat_history):\n",
        "    if audio is None:\n",
        "        return None, chat_history\n",
        "    print(audio)\n",
        "    message_transcription = convert_to_text(audio)\n",
        "    _, chat_history = run_text_prompt(message_transcription, chat_history)\n",
        "    return None, chat_history\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(label=\"Chat with Phi 3 mini 4k instruct\")\n",
        "\n",
        "    msg = gr.Textbox(label=\"Ask anything\")\n",
        "    msg.submit(run_text_prompt, [msg, chatbot], [msg, chatbot])\n",
        "\n",
        "    with gr.Row():\n",
        "        audio = gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
        "\n",
        "        send_audio_button = gr.Button(\"Send Audio\", interactive=True)\n",
        "        send_audio_button.click(run_audio_prompt, [audio, chatbot], [audio, chatbot])\n",
        "\n",
        "demo.launch(share=True,debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
