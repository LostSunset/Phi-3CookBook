Dans le contexte de Phi-3-mini, l'inférence fait référence au processus d'utilisation du modèle pour faire des prédictions ou générer des résultats basés sur des données d'entrée. Laissez-moi vous donner plus de détails sur Phi-3-mini et ses capacités d'inférence.

Phi-3-mini fait partie de la série de modèles Phi-3 publiée par Microsoft. Ces modèles sont conçus pour redéfinir ce qui est possible avec les Small Language Models (SLMs).

Voici quelques points clés sur Phi-3-mini et ses capacités d'inférence :

## **Aperçu de Phi-3-mini :**
- Phi-3-mini a une taille de paramètre de 3,8 milliards.
- Il peut fonctionner non seulement sur des dispositifs informatiques traditionnels, mais aussi sur des dispositifs edge tels que les appareils mobiles et les dispositifs IoT.
- La sortie de Phi-3-mini permet aux individus et aux entreprises de déployer des SLMs sur différents dispositifs matériels, notamment dans des environnements à ressources limitées.
- Il couvre divers formats de modèles, y compris le format PyTorch traditionnel, la version quantifiée du format gguf et la version quantifiée basée sur ONNX.

## **Accéder à Phi-3-mini :**
Pour accéder à Phi-3-mini, vous pouvez utiliser [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) dans une application Copilot. Semantic Kernel est généralement compatible avec Azure OpenAI Service, les modèles open-source sur Hugging Face, et les modèles locaux.
Vous pouvez également utiliser [Ollama](https://ollama.com) ou [LlamaEdge](https://llamaedge.com) pour appeler des modèles quantifiés. Ollama permet aux utilisateurs individuels d'appeler différents modèles quantifiés, tandis que LlamaEdge offre une disponibilité multiplateforme pour les modèles GGUF.

## **Modèles quantifiés :**
De nombreux utilisateurs préfèrent utiliser des modèles quantifiés pour l'inférence locale. Par exemple, vous pouvez exécuter directement Ollama run Phi-3 ou le configurer hors ligne en utilisant un Modelfile. Le Modelfile spécifie le chemin du fichier GGUF et le format de l'invite.

## **Possibilités de l'IA générative :**
Combiner des SLMs comme Phi-3-mini ouvre de nouvelles possibilités pour l'IA générative. L'inférence n'est que la première étape ; ces modèles peuvent être utilisés pour diverses tâches dans des scénarios à ressources limitées, à latence contrainte et à coût réduit.

## **Déverrouiller l'IA générative avec Phi-3-mini : Un guide pour l'inférence et le déploiement**
Apprenez à utiliser Semantic Kernel, Ollama/LlamaEdge et ONNX Runtime pour accéder aux modèles Phi-3-mini et en faire l'inférence, et explorez les possibilités de l'IA générative dans divers scénarios d'application.

**Caractéristiques**
Inférence du modèle phi3-mini dans :

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

En résumé, Phi-3-mini permet aux développeurs d'explorer différents formats de modèles et de tirer parti de l'IA générative dans divers scénarios d'application.

Avertissement : La traduction a été réalisée à partir de son original par un modèle d'IA et peut ne pas être parfaite. Veuillez examiner le résultat et apporter les corrections nécessaires.