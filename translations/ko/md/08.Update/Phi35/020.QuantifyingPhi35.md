# **Phi-3.5 패밀리 정량화**

모델 정량화는 신경망 모델의 파라미터(예: 가중치 및 활성화 값)를 큰 값 범위(일반적으로 연속 값 범위)에서 작은 유한 값 범위로 매핑하는 과정을 말합니다. 이 기술은 모델의 크기와 계산 복잡성을 줄이고 모바일 장치나 임베디드 시스템과 같은 자원이 제한된 환경에서 모델의 운영 효율성을 향상시킬 수 있습니다. 모델 정량화는 파라미터의 정밀도를 줄여 압축을 달성하지만, 이는 일정한 정밀도 손실도 가져옵니다. 따라서 정량화 과정에서는 모델 크기, 계산 복잡성 및 정밀도 간의 균형을 맞추는 것이 필요합니다. 일반적인 정량화 방법에는 고정 소수점 정량화, 부동 소수점 정량화 등이 있으며, 특정 상황과 필요에 따라 적절한 정량화 전략을 선택할 수 있습니다.

우리는 GenAI 모델을 엣지 디바이스에 배포하여 모바일 디바이스, AI PC/Copilot+PC, 전통적인 IoT 디바이스 등 다양한 디바이스가 GenAI 시나리오에 진입할 수 있도록 하고자 합니다. 정량화된 모델을 통해 다양한 엣지 디바이스에 맞춰 배포할 수 있습니다. 하드웨어 제조업체가 제공하는 모델 가속 프레임워크와 정량화된 모델을 결합하여 더 나은 SLM 응용 시나리오를 구축할 수 있습니다.

정량화 시나리오에서는 다양한 정밀도(INT4, INT8, FP16, FP32)를 사용합니다. 다음은 일반적으로 사용되는 정량화 정밀도에 대한 설명입니다.

### **INT4**

INT4 정량화는 모델의 가중치와 활성화 값을 4비트 정수로 정량화하는 급진적인 정량화 방법입니다. INT4 정량화는 표현 범위가 작고 정밀도가 낮기 때문에 일반적으로 더 큰 정밀도 손실을 초래합니다. 그러나 INT8 정량화에 비해 INT4 정량화는 모델의 저장 요구 사항과 계산 복잡성을 더욱 줄일 수 있습니다. 실용적인 응용에서는 INT4 정량화가 상대적으로 드물게 사용됩니다. 이는 너무 낮은 정확도가 모델 성능 저하를 유발할 수 있기 때문입니다. 또한, 모든 하드웨어가 INT4 연산을 지원하지 않으므로 정량화 방법을 선택할 때 하드웨어 호환성을 고려해야 합니다.

### **INT8**

INT8 정량화는 모델의 가중치와 활성화를 부동 소수점 숫자에서 8비트 정수로 변환하는 과정입니다. INT8 정수로 표현되는 수치 범위는 작고 정밀도가 낮지만, 저장 및 계산 요구 사항을 크게 줄일 수 있습니다. INT8 정량화에서는 모델의 가중치와 활성화 값이 원래 부동 소수점 정보를 최대한 보존하기 위해 스케일링 및 오프셋을 포함한 정량화 과정을 거칩니다. 추론 중에는 이러한 정량화된 값을 계산을 위해 다시 부동 소수점 숫자로 디정량화한 다음, 다음 단계로 다시 INT8로 정량화합니다. 이 방법은 대부분의 응용에서 충분한 정확도를 제공하면서 높은 계산 효율성을 유지할 수 있습니다.

### **FP16**

FP16 형식, 즉 16비트 부동 소수점 숫자(float16)는 32비트 부동 소수점 숫자(float32)에 비해 메모리 사용량을 절반으로 줄여 대규모 딥러닝 응용에서 중요한 이점을 제공합니다. FP16 형식은 동일한 GPU 메모리 제한 내에서 더 큰 모델을 로드하거나 더 많은 데이터를 처리할 수 있게 합니다. 현대 GPU 하드웨어가 계속해서 FP16 연산을 지원함에 따라, FP16 형식을 사용하면 계산 속도도 향상될 수 있습니다. 그러나 FP16 형식은 낮은 정밀도를 가지고 있어, 일부 경우에는 수치 불안정성이나 정밀도 손실을 초래할 수 있습니다.

### **FP32**

FP32 형식은 높은 정밀도를 제공하며 넓은 범위의 값을 정확하게 표현할 수 있습니다. 복잡한 수학적 연산을 수행하거나 높은 정밀도의 결과가 필요한 시나리오에서는 FP32 형식이 선호됩니다. 그러나 높은 정확도는 더 많은 메모리 사용과 더 긴 계산 시간을 의미합니다. 특히 모델 파라미터가 많고 데이터 양이 방대한 대규모 딥러닝 모델의 경우, FP32 형식은 GPU 메모리 부족이나 추론 속도 저하를 초래할 수 있습니다.

모바일 디바이스나 IoT 디바이스에서는 Phi-3.x 모델을 INT4로 변환할 수 있으며, AI PC / Copilot PC는 INT8, FP16, FP32와 같은 더 높은 정밀도를 사용할 수 있습니다.

현재 다양한 하드웨어 제조업체는 Intel의 OpenVINO, Qualcomm의 QNN, Apple의 MLX, Nvidia의 CUDA 등과 같은 프레임워크를 통해 생성 모델을 지원하며, 모델 정량화를 결합하여 로컬 배포를 완료합니다.

기술적으로는 정량화 후 다양한 형식 지원이 있습니다. 예를 들어 PyTorch / Tensorflow 형식, GGUF, ONNX 등이 있습니다. GGUF와 ONNX의 형식 비교 및 응용 시나리오를 수행했습니다. 여기서는 모델 프레임워크에서 하드웨어까지 좋은 지원을 제공하는 ONNX 정량화 형식을 추천합니다. 이 장에서는 ONNX Runtime for GenAI, OpenVINO, Apple MLX를 사용하여 모델 정량화를 수행할 것입니다. (더 나은 방법이 있다면 PR을 제출하여 알려주십시오)

**이 장에는 다음 내용이 포함됩니다**

1. [llama.cpp를 사용하여 Phi-3.5 정량화](./021.UsingLlamacppQuantifyingPhi35.md)

2. [onnxruntime을 위한 Generative AI 확장을 사용하여 Phi-3.5 정량화](./022.UsingORTGenAIQuantifyingPhi35.md)

3. [Intel OpenVINO를 사용하여 Phi-3.5 정량화](./023.UsingIntelOpenVINOQuantifyingPhi35.md)

4. [Apple MLX 프레임워크를 사용하여 Phi-3.5 정량화](./024.UsingAppleMLXQuantifyingPhi35.md)

**면책 조항**:
이 문서는 기계 기반 AI 번역 서비스를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있지만, 자동 번역에는 오류나 부정확한 내용이 포함될 수 있습니다. 원어로 작성된 원본 문서를 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 오역에 대해 당사는 책임을 지지 않습니다.