# **onnxruntime用生成AI拡張機能を使ってPhi-3.5を量子化する**

## **onnxruntime用生成AI拡張機能とは**

この拡張機能は、ONNX Runtime（[https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai)）で生成AIを実行するのに役立ちます。ONNXモデルに対する生成AIループを提供し、ONNX Runtimeによる推論、ロジット処理、検索とサンプリング、KVキャッシュ管理を含みます。開発者は高レベルのgenerate()メソッドを呼び出すか、モデルの各イテレーションをループで実行し、トークンを一つずつ生成し、オプションでループ内で生成パラメータを更新することができます。グリーディ/ビームサーチとTopP、TopKサンプリングによるトークンシーケンス生成と、繰り返しペナルティなどの組み込みロジット処理をサポートしています。カスタムスコアリングも簡単に追加できます。

アプリケーションレベルでは、onnxruntime用生成AI拡張機能を使用して、C++/C#/Pythonでアプリケーションを構築できます。モデルレベルでは、微調整されたモデルをマージしたり、関連する量的デプロイメント作業を行うことができます。


## **onnxruntime用生成AI拡張機能でPhi-3.5を量子化する**

### **サポートモデル**

onnxruntime用生成AI拡張機能は、Microsoft Phi、Google Gemma、Mistral、Meta LLaMAの量子化変換をサポートしています。


### **onnxruntime用生成AI拡張機能のモデルビルダー**

モデルビルダーは、ONNX Runtime generate() APIで動作する最適化および量子化されたONNXモデルの作成を大幅に加速します。

モデルビルダーを通じて、モデルをINT4、INT8、FP16、FP32に量子化し、CPU、CUDA、DirectML、Mobileなどのさまざまなハードウェアアクセラレーション方法を組み合わせることができます。

モデルビルダーを使用するには、以下をインストールする必要があります。

```bash
pip install torch transformers onnx onnxruntime
pip install --pre onnxruntime-genai
```

インストール後、端末からモデルビルダースクリプトを実行して、モデルの形式と量子化変換を行うことができます。

```bash
python3 -m onnxruntime_genai.models.builder -m model_name -o path_to_output_folder -p precision -e execution_provider -c cache_dir_to_save_hf_files
```

関連パラメータの理解

1. **model_name** これはHugging Face上のモデルで、例えばmicrosoft/Phi-3.5-mini-instructやmicrosoft/Phi-3.5-vision-instructなどです。また、あなたがモデルを保存しているパスでもかまいません。

2. **path_to_output_folder** 量子化変換の保存パス

3. **execution_provider** 異なるハードウェアアクセラレーションのサポート、例えばcpu、cuda、DirectML

4. **cache_dir_to_save_hf_files** Hugging Faceからモデルをダウンロードし、ローカルにキャッシュします


***Note：***

## **モデルビルダーを使ってPhi-3.5を量子化する方法**

モデルビルダーは現在、Phi-3.5 InstructとPhi-3.5-VisionのONNXモデルの量子化をサポートしています。

### **Phi-3.5-Instruct**

**CPU加速によるINT4の量子化変換**

```bash
python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cpu -c ./Phi-3.5-mini-instruct
```

**CUDA加速によるINT4の量子化変換**

```bash
python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct
```

```python
python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct
```

### **Phi-3.5-Vision**

**Phi-3.5-vision-instruct-onnx-cpu-fp32**

1. ターミナルで環境を設定

```bash
mkdir models
cd models 
```

2. modelsフォルダーにmicrosoft/Phi-3.5-vision-instructをダウンロード
[https://huggingface.co/microsoft/Phi-3.5-vision-instruct](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)

3. これらのファイルをPhi-3.5-vision-instructフォルダーにダウンロード

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py)

4. このファイルをmodelsフォルダーにダウンロード
[https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py)

5. ターミナルに移動

    FP32でONNXサポートに変換

```bash
python build.py -i .\Your Phi-3.5-vision-instruct Path\ -o .\vision-cpu-fp32 -p f32 -e cpu
```

### **Note：**

1. モデルビルダーは現在、Phi-3.5-InstructとPhi-3.5-Visionの変換をサポートしていますが、Phi-3.5-MoEはサポートしていません。

2. ONNXの量子化モデルを使用するには、onnxruntime用生成AI拡張機能SDKを通じて使用できます。

3. より責任あるAIを考慮するため、モデルの量子化変換後により効果的な結果テストを行うことをお勧めします。

4. CPU INT4モデルを量子化することで、エッジデバイスにデプロイでき、より良いアプリケーションシナリオを持つことができます。そのため、Phi-3.5-InstructをINT4周りで完了しました。

## **リソース**

1. onnxruntime用生成AI拡張機能について詳しく知る [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. onnxruntime用生成AI拡張機能のGitHubリポジトリ [https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai)

免責事項: この翻訳はAIモデルによって原文から翻訳されたものであり、完璧ではない可能性があります。 出力を確認し、必要な修正を行ってください。