# **Quantification de Phi-3.5 à l'aide des extensions d'IA générative pour onnxruntime**

## **Qu'est-ce que les extensions d'IA générative pour onnxruntime**

Ces extensions vous aident à exécuter l'IA générative avec ONNX Runtime ([https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai)). Elles fournissent la boucle d'IA générative pour les modèles ONNX, y compris l'inférence avec ONNX Runtime, le traitement des logits, la recherche et l'échantillonnage, ainsi que la gestion du cache KV. Les développeurs peuvent appeler une méthode generate() de haut niveau, ou exécuter chaque itération du modèle dans une boucle, générant un token à la fois, et éventuellement mettre à jour les paramètres de génération à l'intérieur de la boucle. Il prend en charge la recherche gloutonne/à faisceau et l'échantillonnage TopP, TopK pour générer des séquences de tokens et le traitement intégré des logits comme les pénalités de répétition. Vous pouvez également ajouter facilement une notation personnalisée.

Au niveau de l'application, vous pouvez utiliser les extensions d'IA générative pour onnxruntime pour créer des applications en utilisant C++/C#/Python. Au niveau du modèle, vous pouvez les utiliser pour fusionner des modèles ajustés et effectuer des travaux de déploiement quantitatif connexes.


## **Quantification de Phi-3.5 avec les extensions d'IA générative pour onnxruntime**

### **Modèles pris en charge**

Les extensions d'IA générative pour onnxruntime prennent en charge la conversion quantifiée de Microsoft Phi, Google Gemma, Mistral, Meta LLaMA.


### **Model Builder dans les extensions d'IA générative pour onnxruntime**

Le Model Builder accélère grandement la création de modèles ONNX optimisés et quantifiés qui s'exécutent avec l'API generate() de ONNX Runtime.

Grâce à Model Builder, vous pouvez quantifier le modèle en INT4, INT8, FP16, FP32, et combiner différentes méthodes d'accélération matérielle telles que CPU, CUDA, DirectML, Mobile, etc.

Pour utiliser Model Builder, vous devez installer

```bash

pip install torch transformers onnx onnxruntime

pip install --pre onnxruntime-genai

```

Après l'installation, vous pouvez exécuter le script Model Builder depuis le terminal pour effectuer la conversion de format et de quantification du modèle.


```bash

python3 -m onnxruntime_genai.models.builder -m model_name -o path_to_output_folder -p precision -e execution_provider -c cache_dir_to_save_hf_files

```

Comprendre les paramètres pertinents

1. **model_name** C'est le modèle sur Hugging face, tel que microsoft/Phi-3.5-mini-instruct, microsoft/Phi-3.5-vision-instruct, etc. Il peut aussi être le chemin où vous stockez le modèle.

2. **path_to_output_folder** Chemin de sauvegarde de la conversion quantifiée.

3. **execution_provider** Différents supports d'accélération matérielle, tels que cpu, cuda, DirectML.

4. **cache_dir_to_save_hf_files** Nous téléchargeons le modèle depuis Hugging face et le mettons en cache localement.




***Note：***

## **Comment utiliser Model Builder pour quantifier Phi-3.5**

Model Builder prend désormais en charge la quantification des modèles ONNX pour Phi-3.5 Instruct et Phi-3.5-Vision

### **Phi-3.5-Instruct**


**Conversion accélérée par CPU en INT 4 quantifié**


```bash

python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cpu -c ./Phi-3.5-mini-instruct

```

**Conversion accélérée par CUDA en INT 4 quantifié**

```bash

python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct

```



```python

python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct  -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct

```


### **Phi-3.5-Vision**

**Phi-3.5-vision-instruct-onnx-cpu-fp32**

1. Configurer l'environnement dans le terminal

```bash

mkdir models

cd models 

```

2. Télécharger microsoft/Phi-3.5-vision-instruct dans le dossier models
[https://huggingface.co/microsoft/Phi-3.5-vision-instruct](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)

3. Veuillez télécharger ces fichiers dans votre dossier Phi-3.5-vision-instruct

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py)


4. Télécharger ce fichier dans le dossier models
[https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py)

5. Aller au terminal

    Convertir la prise en charge ONNX avec FP32


```bash

python build.py -i .\Your Phi-3.5-vision-instruct Path\ -o .\vision-cpu-fp32 -p f32 -e cpu

```


### **Note：**

1. Model Builder prend actuellement en charge la conversion de Phi-3.5-Instruct et Phi-3.5-Vision, mais pas de Phi-3.5-MoE

2. Pour utiliser le modèle quantifié d'ONNX, vous pouvez l'utiliser via le SDK des extensions d'IA générative pour onnxruntime

3. Nous devons envisager une IA plus responsable, donc après la conversion quantifiée du modèle, il est recommandé de réaliser des tests de résultats plus efficaces

4. En quantifiant le modèle CPU INT4, nous pouvons le déployer sur un dispositif Edge, ce qui offre de meilleurs scénarios d'application, donc nous avons complété Phi-3.5-Instruct autour de INT 4


## **Ressources**

1. En savoir plus sur les extensions d'IA générative pour onnxruntime [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. Dépôt GitHub des extensions d'IA générative pour onnxruntime [https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai)

Avertissement : La traduction a été réalisée à partir de l'original par un modèle d'IA et peut ne pas être parfaite. Veuillez examiner le résultat et apporter les corrections nécessaires.