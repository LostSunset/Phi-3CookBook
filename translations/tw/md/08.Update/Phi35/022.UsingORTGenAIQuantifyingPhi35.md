# **使用 Generative AI 擴展進行 Phi-3.5 量化**

## **什麼是 Generative AI 擴展**

這個擴展幫助你在 ONNX Runtime 上運行生成式 AI（[https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai)）。它為 ONNX 模型提供了生成式 AI 的循環，包括使用 ONNX Runtime 進行推理、logits 處理、搜索和採樣以及 KV 緩存管理。開發者可以調用高級的 generate() 方法，或者在循環中運行模型的每次迭代，一次生成一個 token，並在循環內可選地更新生成參數。它支持貪婪/束搜索和 TopP、TopK 採樣來生成 token 序列，並內置了重複懲罰等 logits 處理。你還可以輕鬆添加自定義評分。

在應用層面，你可以使用 Generative AI 擴展來構建使用 C++/C#/Python 的應用程序。在模型層面，你可以用它來合併微調模型並進行相關的量化部署工作。

## **使用 Generative AI 擴展進行 Phi-3.5 量化**

### **支持的模型**

Generative AI 擴展支持 Microsoft Phi、Google Gemma、Mistral、Meta LLaMA 的量化轉換。

### **Generative AI 擴展中的模型構建器**

模型構建器大大加快了創建運行於 ONNX Runtime generate() API 的優化和量化 ONNX 模型的速度。

通過模型構建器，你可以將模型量化到 INT4、INT8、FP16、FP32，並結合不同的硬件加速方法，如 CPU、CUDA、DirectML、Mobile 等。

使用模型構建器，你需要安裝：

```bash
pip install torch transformers onnx onnxruntime

pip install --pre onnxruntime-genai
```

安裝完成後，你可以從終端運行模型構建器腳本來進行模型格式和量化轉換。

```bash
python3 -m onnxruntime_genai.models.builder -m model_name -o path_to_output_folder -p precision -e execution_provider -c cache_dir_to_save_hf_files
```

理解相關參數：

1. **model_name** 這是 Hugging face 上的模型，例如 microsoft/Phi-3.5-mini-instruct、microsoft/Phi-3.5-vision-instruct 等。也可以是你存儲模型的路徑。

2. **path_to_output_folder** 量化轉換的保存路徑。

3. **execution_provider** 不同的硬件加速支持，例如 cpu、cuda、DirectML。

4. **cache_dir_to_save_hf_files** 我們從 Hugging face 下載模型並在本地緩存。

***注意：***

## **如何使用模型構建器量化 Phi-3.5**

模型構建器現在支持 Phi-3.5 Instruct 和 Phi-3.5-Vision 的 ONNX 模型量化。

### **Phi-3.5-Instruct**

**CPU 加速的 INT 4 量化轉換**

```bash
python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct -o ./onnx-cpu -p int4 -e cpu -c ./Phi-3.5-mini-instruct
```

**CUDA 加速的 INT 4 量化轉換**

```bash
python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct
```

```python
python3 -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct -o ./onnx-cpu -p int4 -e cuda -c ./Phi-3.5-mini-instruct
```

### **Phi-3.5-Vision**

**Phi-3.5-vision-instruct-onnx-cpu-fp32**

1. 在終端設置環境

```bash
mkdir models

cd models
```

2. 在 models 文件夾中下載 microsoft/Phi-3.5-vision-instruct
[https://huggingface.co/microsoft/Phi-3.5-vision-instruct](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)

3. 請下載這些文件到你的 Phi-3.5-vision-instruct 文件夾

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/resolve/main/onnx/config.json)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/image_embedding_phi3_v_for_onnx.py)

- [https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/modeling_phi3_v.py)

4. 下載這個文件到 models 文件夾
[https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py](https://huggingface.co/lokinfey/Phi-3.5-vision-instruct-onnx-cpu/blob/main/onnx/build.py)

5. 打開終端

    使用 FP32 進行 ONNX 支持轉換

```bash
python build.py -i .\Your Phi-3.5-vision-instruct Path\ -o .\vision-cpu-fp32 -p f32 -e cpu
```

### **注意：**

1. 模型構建器目前支持 Phi-3.5-Instruct 和 Phi-3.5-Vision 的轉換，但不支持 Phi-3.5-MoE。

2. 要使用 ONNX 的量化模型，你可以通過 Generative AI 擴展 for onnxruntime SDK 來使用。

3. 我們需要考慮更多的負責任 AI，因此在模型量化轉換後，建議進行更有效的結果測試。

4. 通過量化 CPU INT4 模型，我們可以將其部署到邊緣設備，這樣有更好的應用場景，因此我們已經完成了圍繞 INT 4 的 Phi-3.5-Instruct。

## **資源**

1. 了解更多關於 Generative AI 擴展的信息 [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. Generative AI 擴展的 GitHub 倉庫 [https://github.com/microsoft/onnxruntime-genai](https://github.com/microsoft/onnxruntime-genai)

免責聲明：此翻譯是由AI模型從原文翻譯而來，可能不完全準確。請檢查輸出並進行任何必要的修正。