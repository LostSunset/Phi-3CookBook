# Utilisation du GPU Windows pour créer une solution Prompt flow avec Phi-3.5-Instruct ONNX 

Le document suivant est un exemple de comment utiliser PromptFlow avec ONNX (Open Neural Network Exchange) pour développer des applications IA basées sur les modèles Phi-3.

PromptFlow est une suite d'outils de développement conçue pour rationaliser le cycle de développement complet des applications IA basées sur les LLM (Large Language Model), de l'idéation et du prototypage aux tests et à l'évaluation.

En intégrant PromptFlow avec ONNX, les développeurs peuvent :

- Optimiser les performances du modèle : Utilisez ONNX pour une inférence et un déploiement efficaces du modèle.
- Simplifier le développement : Utilisez PromptFlow pour gérer le flux de travail et automatiser les tâches répétitives.
- Améliorer la collaboration : Facilitez une meilleure collaboration entre les membres de l'équipe en fournissant un environnement de développement unifié.

**Prompt flow** est une suite d'outils de développement conçue pour rationaliser le cycle de développement complet des applications IA basées sur les LLM, de l'idéation, du prototypage, des tests, de l'évaluation au déploiement en production et à la surveillance. Il rend l'ingénierie des prompts beaucoup plus facile et vous permet de créer des applications LLM de qualité production.

Prompt flow peut se connecter à OpenAI, Azure OpenAI Service, et des modèles personnalisables (Huggingface, LLM/SLM locaux). Nous espérons déployer le modèle ONNX quantifié de Phi-3.5 dans des applications locales. Prompt flow peut nous aider à mieux planifier notre entreprise et à compléter des solutions locales basées sur Phi-3.5. Dans cet exemple, nous combinerons la bibliothèque ONNX Runtime GenAI pour compléter la solution Prompt flow basée sur le GPU Windows.

## **Installation**

### **ONNX Runtime GenAI pour GPU Windows**

Lisez cette directive pour configurer ONNX Runtime GenAI pour GPU Windows [cliquez ici](./041.ORTWindowGPUGuideline.md)

### **Configurer Prompt flow dans VSCode**

1. Installer l'extension VS Code Prompt flow

![pfvscode](../../../../../translated_images/pfvscode.6a83e68c73bb1e43a139552bdf1225de2d2da09f5a3f6dbea12c5eedf7b2b346.fr.png)

2. Après avoir installé l'extension VS Code Prompt flow, cliquez sur l'extension, et choisissez **Installation dependencies** suivez cette directive pour installer le SDK Prompt flow dans votre environnement

![pfsetup](../../../../../translated_images/pfsetup.38f996cdb03932b973908d6f3488b3739c4175fe8aeb054824777b5aa5dee9b6.fr.png)

3. Téléchargez [Sample Code](../../../../../code/09.UpdateSamples/Aug/pf/onnx_inference_pf) et utilisez VS Code pour ouvrir cet exemple

![pfsample](../../../../../translated_images/pfsample.c6d81e3718c262befa23d8e77930b4711bd0a15a8dad5f11b1a079254db802ab.fr.png)

4. Ouvrez **flow.dag.yaml** pour choisir votre environnement Python

![pfdag](../../../../../translated_images/pfdag.7db581854ff4c2e201ffbea9798b8c22fb12bb1ace133317246e9b2adaad7cb9.fr.png)

   Ouvrez **chat_phi3_ort.py** pour changer l'emplacement de votre modèle Phi-3.5-instruct ONNX

![pfphi](../../../../../translated_images/pfphi.447df74b48099f692f3c1964b32a98b1dbb8270781f3bd7dc71a089a386a3124.fr.png)

5. Exécutez votre prompt flow pour tester

Ouvrez **flow.dag.yaml** cliquez sur l'éditeur visuel

![pfv](../../../../../translated_images/pfv.3a0fe62a5a8ca695864ac433c9b2ffb4825a4190caf8c372c06aa281cdef76d0.fr.png)

après avoir cliqué dessus, et exécutez-le pour tester

![pfflow](../../../../../translated_images/pfflow.613bbe61a2c9390d66fac767fdea62fc095372a228edddefa68a9cffdb266ca0.fr.png)

6. Vous pouvez exécuter un batch dans le terminal pour vérifier plus de résultats


```bash

pf run create --file batch_run.yaml --stream --name 'Your eval qa name'    

```

Vous pouvez vérifier les résultats dans votre navigateur par défaut


![pfresult](../../../../../translated_images/pfresult.d9c3d7889d3f9249a3e264f25d3016d2b15352ce1f8aab00887b1403212b28bb.fr.png)

**Avertissement** :
Ce document a été traduit à l'aide de services de traduction automatique basés sur l'IA. Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interprétations erronées résultant de l'utilisation de cette traduction.