# **Quoi de neuf dans la famille Phi-3.5**

Utilisez-vous d√©j√† la famille Phi-3 ? Quels sont vos sc√©narios ? Le 20 ao√ªt 2024, Microsoft a lanc√© la nouvelle famille Phi-3.5, qui a √©t√© am√©lior√©e en multilingue, vision et Agent IA. Faisons une introduction plus d√©taill√©e en nous basant sur la Model Card sur Hugging Face.

![PhiFamily](../../../../../translated_images/Phi3getstarted.086dfb90bb69325da6b717586337f2aec5decc241fda85e322eb55c709167f73.fr.png)


## **Phi-3.5-mini-instruct**

Phi-3.5-mini est un mod√®le l√©ger et √† la pointe de la technologie, construit √† partir de jeux de donn√©es utilis√©s pour Phi-3 - donn√©es synth√©tiques et sites web publics filtr√©s - avec un accent sur des donn√©es de tr√®s haute qualit√© et riches en raisonnement. Le mod√®le appartient √† la famille des mod√®les Phi-3 et prend en charge une longueur de contexte de 128K tokens. Le mod√®le a subi un processus d'am√©lioration rigoureux, int√©grant √† la fois un ajustement supervis√©, une optimisation de la politique proximale et une optimisation des pr√©f√©rences directes pour garantir une adh√©sion pr√©cise aux instructions et des mesures de s√©curit√© robustes.

![benchmark1](../../../../../translated_images/benchmark1.479cb048e7d9239b09e562c410a54f6c9eaf85030af67ac6e7de80a69e4778a5.fr.png)

![benchmark2](../../../../../translated_images/benchmark2.76982d411a07caa3ebd706dd6c0ba98b98a5609de371176a67cd619d70d4e6da.fr.png)

√Ä travers les indicateurs du Benchmark, vous pouvez voir que Phi-3.5-mini a am√©lior√© le support pour plusieurs langues et contenus textuels longs par rapport √† Phi-3-mini, ce qui est utilis√© pour am√©liorer les capacit√©s linguistiques et textuelles de Phi-3.5 mini dans les applications en p√©riph√©rie.

Nous pouvons comparer la capacit√© de connaissance en chinois via les mod√®les GitHub. Lorsque nous demandons "O√π se trouve Changsha?" (ÈïøÊ≤ôÂú®Âì™Èáå?), nous pouvons comparer les r√©sultats de Phi-3-mini-128k-instruct et Phi-3.5-mini-128k-instruct.

![Phi3](../../../../../translated_images/gh3.6b1a5c38ed732e40c0effaf4c558badfab0be6148b194aa6bec44adbfb1e4342.fr.png)

![Phi35](../../../../../translated_images/gh35.b0fd2ff379a5f2d995ea1faedd2d7260cfcad7ffbad5a721a8a1b2b3d84028c8.fr.png)

Il n'est pas difficile de voir que l'am√©lioration des donn√©es sur le corpus chinois permet √† Phi-3.5-mini d'obtenir de meilleurs r√©sultats dans les sc√©narios de g√©n√©ration de texte de base (***Note:*** Veuillez noter que si Phi-3.5-mini n√©cessite une r√©ponse plus pr√©cise, il est recommand√© de l'affiner en fonction du sc√©nario d'application).

## **Phi-3.5-vision-instruct**

Phi-3.5-vision est un mod√®le multimodal l√©ger et √† la pointe de la technologie, construit √† partir de jeux de donn√©es comprenant des donn√©es synth√©tiques et des sites web publics filtr√©s, avec un accent sur des donn√©es de tr√®s haute qualit√© et riches en raisonnement, tant en texte qu'en vision. Le mod√®le appartient √† la famille des mod√®les Phi-3, et la version multimodale prend en charge une longueur de contexte de 128K tokens. Le mod√®le a subi un processus d'am√©lioration rigoureux, int√©grant √† la fois un ajustement supervis√© et une optimisation des pr√©f√©rences directes pour garantir une adh√©sion pr√©cise aux instructions et des mesures de s√©curit√© robustes.

Gr√¢ce √† Vision, nous avons ouvert les yeux de la famille Phi-3.x et avons pu compl√©ter les sc√©narios suivants :

1. Environnements √† m√©moire/puissance de calcul limit√©e
2. Sc√©narios li√©s √† la latence
3. Compr√©hension g√©n√©rale des images
4. Reconnaissance optique de caract√®res
5. Compr√©hension des graphiques et des tableaux
6. Comparaison de multiples images
7. R√©sum√© de plusieurs images ou clips vid√©o

Gr√¢ce √† Vision, nous avons permis √† la famille Phi d'ouvrir les yeux et de compl√©ter les sc√©narios suivants.

Nous pouvons √©galement utiliser le benchmark fourni par Hugging Face pour comprendre la comparaison dans diff√©rents sc√©narios visuels.

![benchmark3](../../../../../translated_images/benchmark3.4d9484cc062f0c5076783f3cb33fe533c03995d3a5debc437420e88960032672.fr.png)

Si vous souhaitez essayer gratuitement Phi-3.5-vision-instruct, nous pouvons utiliser [Nivida NIM](https://build.nvidia.com/microsoft/phi-3_5-vision-instruct) pour compl√©ter l'exp√©rience.

![nim](../../../../../translated_images/nim.c985945596d6b2629658087485d16028a3874dcc37329de51b94adf09d0af661.fr.png)

Bien s√ªr, vous pouvez √©galement compl√©ter le d√©ploiement via Azure AI Studio.

## **Phi-3.5-MoE-instruct**

Phi-3.5-MoE est un mod√®le l√©ger et √† la pointe de la technologie, construit √† partir de jeux de donn√©es utilis√©s pour Phi-3 - donn√©es synth√©tiques et documents publics filtr√©s - avec un accent sur des donn√©es de tr√®s haute qualit√© et riches en raisonnement. Le mod√®le prend en charge le multilingue et dispose d'une longueur de contexte de 128K tokens. Le mod√®le a subi un processus d'am√©lioration rigoureux, int√©grant un ajustement supervis√©, une optimisation de la politique proximale et une optimisation des pr√©f√©rences directes pour garantir une adh√©sion pr√©cise aux instructions et des mesures de s√©curit√© robustes.

Avec le d√©veloppement de l'Agent IA, la demande pour les mod√®les MoE augmentera progressivement. MoE, dont le nom complet est Mixed Expert Models, est un nouveau mod√®le form√© en m√©langeant plusieurs mod√®les experts. MOE consiste √† diviser d'abord le gros probl√®me, puis √† r√©soudre les petits probl√®mes un par un, puis √† r√©sumer les conclusions. Deuxi√®mement, l'√©chelle du mod√®le est l'un des facteurs cl√©s pour am√©liorer les performances du mod√®le. Avec des ressources informatiques limit√©es, il est souvent pr√©f√©rable de former un mod√®le plus grand avec moins d'√©tapes de formation que de former un mod√®le plus petit avec plus d'√©tapes.

Le mod√®le Phi-3.5-MoE-Instruct n√©cessite plus de puissance de calcul que Phi-3.5-Vision et Phi-3.5-Instruct. Il est recommand√© d'utiliser des m√©thodes bas√©es sur le cloud telles que Azure AI Studio et Nvidia NIM pour l'exp√©rience et l'utilisation.

![nim2](../../../../../translated_images/nim2.ab50cc468e987efe5e87e8b9b2927f751b6d080c4a146129c2133da94b0f781e.fr.png)

### **ü§ñ Exemples pour Phi-3.5 avec Apple MLX**

| Labs    | Pr√©sentation | Aller |
| -------- | ------- |  ------- |
| üöÄ Lab-Pr√©sentation Phi-3.5 Instruct  | Apprenez √† utiliser Phi-3.5 Instruct |  [Aller](../../../../../code/09.UpdateSamples/Aug/phi3-instruct-demo.ipynb)    |
| üöÄ Lab-Pr√©sentation Phi-3.5 Vision (image) | Apprenez √† utiliser Phi-3.5 Vision pour analyser une image |  [Aller](../../../../../code/09.UpdateSamples/Aug/phi3-vision-demo.ipynb)    |
| üöÄ Lab-Pr√©sentation Phi-3.5 MoE   | Apprenez √† utiliser Phi-3.5 Vision pour analyser une image |  [Aller](../../../../../code/09.UpdateSamples/Aug/phi3_moe_demo.ipynb)    |


## **Ressources**

1. Phi Family de Hugging Face [https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)

2. √Ä propos des mod√®les GitHub [https://gh.io/models](https://gh.io/models)

3. √Ä propos d'Azure AI Studio [https://ai.azure.com/](https://ai.azure.com/)

4. √Ä propos de Nvidia NIM [https://build.nvidia.com/explore/discover](https://build.nvidia.com/explore/discover)

Avertissement : La traduction a √©t√© r√©alis√©e √† partir de l'original par un mod√®le d'IA et peut ne pas √™tre parfaite. 
Veuillez v√©rifier le r√©sultat et apporter les corrections n√©cessaires.