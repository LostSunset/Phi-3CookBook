# **Phi-3.5 양자화하기 llama.cpp 사용**

## **llama.cpp란 무엇인가**

llama.cpp는 다양한 대형 언어 모델(LLM)에서 추론을 수행하는 C++로 주로 작성된 오픈 소스 소프트웨어 라이브러리입니다. 주요 목표는 최소한의 설정으로 다양한 하드웨어에서 최첨단 성능을 제공하는 것입니다. 또한 이 라이브러리를 위한 Python 바인딩도 제공되어 텍스트 완성 및 OpenAI 호환 웹 서버를 위한 고수준 API를 제공합니다.

llama.cpp의 주요 목표는 최소한의 설정으로 다양한 하드웨어에서 최첨단 성능을 제공하는 것입니다 - 로컬 및 클라우드 모두에서.

- 의존성 없는 순수 C/C++ 구현
- Apple 실리콘에 최적화 - ARM NEON, Accelerate 및 Metal 프레임워크를 통한 최적화
- x86 아키텍처에 대한 AVX, AVX2 및 AVX512 지원
- 더 빠른 추론 및 메모리 사용 감소를 위한 1.5비트, 2비트, 3비트, 4비트, 5비트, 6비트 및 8비트 정수 양자화
- NVIDIA GPU에서 LLM을 실행하기 위한 맞춤형 CUDA 커널 (HIP을 통한 AMD GPU 지원)
- Vulkan 및 SYCL 백엔드 지원
- 총 VRAM 용량보다 큰 모델을 부분적으로 가속화하기 위한 CPU+GPU 하이브리드 추론

## **llama.cpp를 사용하여 Phi-3.5 양자화하기**

Phi-3.5-Instruct 모델은 llama.cpp를 사용하여 양자화할 수 있지만, Phi-3.5-Vision 및 Phi-3.5-MoE는 아직 지원되지 않습니다. llama.cpp에 의해 변환된 형식은 gguf이며, 이는 가장 널리 사용되는 양자화 형식입니다.

Hugging face에는 많은 수의 양자화된 GGUF 형식 모델이 있습니다. AI Foundry, Ollama 및 LlamaEdge는 llama.cpp를 사용하므로 GGUF 모델도 자주 사용됩니다.

### **GGUF란 무엇인가**

GGUF는 모델을 빠르게 로드하고 저장하는 데 최적화된 바이너리 형식으로, 추론 목적에 매우 효율적입니다. GGUF는 GGML 및 기타 실행 엔진과 함께 사용하도록 설계되었습니다. GGUF는 llama.cpp의 개발자인 @ggerganov에 의해 개발되었습니다. 처음에 PyTorch와 같은 프레임워크에서 개발된 모델은 이러한 엔진에서 사용하기 위해 GGUF 형식으로 변환될 수 있습니다.

### **ONNX vs GGUF**

ONNX는 다양한 AI 프레임워크에서 잘 지원되는 전통적인 머신 러닝/딥 러닝 형식이며 엣지 장치에서 좋은 사용 시나리오를 가지고 있습니다. GGUF는 llama.cpp를 기반으로 하며 GenAI 시대에 만들어졌다고 할 수 있습니다. 두 가지는 비슷한 용도를 가지고 있습니다. 임베디드 하드웨어 및 애플리케이션 레이어에서 더 나은 성능을 원한다면 ONNX가 선택일 수 있습니다. llama.cpp의 파생 프레임워크 및 기술을 사용하는 경우 GGUF가 더 나을 수 있습니다.

### **llama.cpp를 사용하여 Phi-3.5-Instruct 양자화하기**

**1. 환경 설정**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. 양자화**

llama.cpp를 사용하여 Phi-3.5-Instruct를 FP16 GGUF로 변환하기


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Phi-3.5를 INT4로 양자화하기


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. 테스트**

llama-cpp-python 설치


```bash

pip install llama-cpp-python -U

```

***참고***

Apple Silicon을 사용하는 경우 다음과 같이 llama-cpp-python을 설치하세요


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

테스트하기


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **리소스**

1. llama.cpp에 대해 더 알아보기 [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. GGUF에 대해 더 알아보기 [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**면책 조항**:
이 문서는 기계 기반 AI 번역 서비스를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있지만 자동 번역에는 오류나 부정확성이 있을 수 있습니다. 원본 문서는 원어로 작성된 문서를 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문 인간 번역을 권장합니다. 이 번역을 사용함으로 인해 발생하는 오해나 잘못된 해석에 대해서는 책임을 지지 않습니다.