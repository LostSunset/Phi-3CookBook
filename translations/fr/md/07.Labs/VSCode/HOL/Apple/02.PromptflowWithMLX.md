# **Lab 2 - Exécuter Prompt flow avec Phi-3-mini dans AIPC**

## **Qu'est-ce que Prompt flow**

Prompt flow est une suite d'outils de développement conçue pour simplifier le cycle de développement complet des applications IA basées sur LLM, de l'idéation, du prototypage, des tests, de l'évaluation au déploiement en production et à la surveillance. Il rend l'ingénierie des prompts beaucoup plus facile et vous permet de construire des applications LLM de qualité production.

Avec Prompt flow, vous pourrez :

- Créer des flux qui lient les LLM, les prompts, le code Python et d'autres outils ensemble dans un workflow exécutable.

- Déboguer et itérer vos flux, en particulier l'interaction avec les LLM, avec facilité.

- Évaluer vos flux, calculer des métriques de qualité et de performance avec des ensembles de données plus larges.

- Intégrer les tests et l'évaluation dans votre système CI/CD pour assurer la qualité de votre flux.

- Déployer vos flux sur la plateforme de service de votre choix ou les intégrer facilement dans le code de votre application.

- (Optionnel mais fortement recommandé) Collaborer avec votre équipe en utilisant la version cloud de Prompt flow dans Azure AI.



## **Construire des flux de génération de code sur Apple Silicon**

***Note*** : Si vous n'avez pas terminé l'installation de l'environnement, veuillez visiter [Lab 0 - Installations](./01.Installations.md)

1. Ouvrez l'extension Prompt flow dans Visual Studio Code et créez un projet de flux vide

![create](../../../../../../../translated_images/pf_create.626fd367cf0ac7981e0731fdfc70fa46df0826f9eaf57c22f07908817ede14d3.fr.png)

2. Ajoutez des paramètres d'entrées et de sorties et ajoutez du code Python comme nouveau flux

![flow](../../../../../../../translated_images/pf_flow.f2d64298a737b204ec7b33604538c97d4fffe9e07e74bad1c162e88e026d3dfa.fr.png)


Vous pouvez vous référer à cette structure (flow.dag.yaml) pour construire votre flux

```yaml

inputs:
  prompt:
    type: string
    default: Écrire du code python pour la série de Fibonacci. Veuillez utiliser markdown comme sortie
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
- name: gen_code_by_phi3
  type: python
  source:
    type: code
    path: gen_code_by_phi3.py
  inputs:
    prompt: ${inputs.prompt}


```

3. Quantifier phi-3-mini

Nous espérons mieux faire fonctionner SLM sur des appareils locaux. En général, nous quantifions le modèle (INT4, FP16, FP32)


```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

**Note:** le dossier par défaut est mlx_model 

4. Ajouter du code dans ***Chat_With_Phi3.py***


```python


from promptflow import tool

from mlx_lm import load, generate


# La section des entrées changera en fonction des arguments de la fonction tool, après avoir enregistré le code
# Ajouter des types aux arguments et à la valeur de retour aidera le système à afficher correctement les types
# Veuillez mettre à jour le nom/la signature de la fonction selon les besoins
@tool
def my_python_tool(prompt: str) -> str:

    model_id = './mlx_model_phi3_mini'

    model, tokenizer = load(model_id)

    # <|user|>\nÉcrire du code python pour la série de Fibonacci. Veuillez utiliser markdown comme sortie<|end|>\n<|assistant|>

    response = generate(model, tokenizer, prompt="<|user|>\n" + prompt  + "<|end|>\n<|assistant|>", max_tokens=2048, verbose=True)

    return response


```

4. Vous pouvez tester le flux depuis Debug ou Run pour vérifier si le code généré est correct ou non 

![RUN](../../../../../../../translated_images/pf_run.57c3f9e7e7052ff85850b8f06648c7d5b4d2ac9f4796381fd8d29b1a41e1f705.fr.png)

5. Exécuter le flux comme API de développement dans le terminal

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Vous pouvez le tester dans Postman / Thunder Client


### **Note**

1. La première exécution prend beaucoup de temps. Il est recommandé de télécharger le modèle phi-3 depuis Hugging face CLI.

2. Compte tenu de la puissance de calcul limitée de l'Intel NPU, il est recommandé d'utiliser Phi-3-mini-4k-instruct

3. Nous utilisons l'accélération Intel NPU pour quantifier la conversion INT4, mais si vous relancez le service, vous devez supprimer les dossiers cache et nc_workshop.



## **Ressources**

1. Apprendre Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Apprendre l'accélération Intel NPU [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Code d'exemple, télécharger [Local NPU Agent Sample Code](../../../../../../../code/07.Lab/01/AIPC/local-npu-agent)

Avertissement : La traduction a été effectuée à partir de son original par un modèle d'IA et peut ne pas être parfaite. Veuillez examiner le résultat et apporter les corrections nécessaires.