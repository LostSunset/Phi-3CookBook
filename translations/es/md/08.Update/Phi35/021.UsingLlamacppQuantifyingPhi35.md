# **Cuantización de Phi-3.5 usando llama.cpp**

## **¿Qué es llama.cpp?**

llama.cpp es una biblioteca de software de código abierto escrita principalmente en C++ que realiza inferencias en varios Modelos de Lenguaje Grande (LLMs), como Llama. Su objetivo principal es proporcionar un rendimiento de vanguardia para la inferencia de LLM en una amplia gama de hardware con una configuración mínima. Además, hay enlaces en Python disponibles para esta biblioteca, que ofrecen una API de alto nivel para la finalización de texto y un servidor web compatible con OpenAI.

El objetivo principal de llama.cpp es permitir la inferencia de LLM con una configuración mínima y un rendimiento de vanguardia en una amplia variedad de hardware, tanto localmente como en la nube.

- Implementación en C/C++ puro sin dependencias
- Apple silicon es un ciudadano de primera clase - optimizado a través de ARM NEON, Accelerate y frameworks Metal
- Soporte AVX, AVX2 y AVX512 para arquitecturas x86
- Cuantización de enteros de 1.5 bits, 2 bits, 3 bits, 4 bits, 5 bits, 6 bits y 8 bits para una inferencia más rápida y uso reducido de memoria
- Kernels CUDA personalizados para ejecutar LLMs en GPUs NVIDIA (soporte para GPUs AMD a través de HIP)
- Soporte de backend Vulkan y SYCL
- Inferencia híbrida CPU+GPU para acelerar parcialmente modelos más grandes que la capacidad total de VRAM

## **Cuantización de Phi-3.5 con llama.cpp**

El modelo Phi-3.5-Instruct puede ser cuantizado usando llama.cpp, pero Phi-3.5-Vision y Phi-3.5-MoE aún no son compatibles. El formato convertido por llama.cpp es gguf, que también es el formato de cuantización más utilizado.

Hay una gran cantidad de modelos en formato GGUF cuantizados en Hugging Face. AI Studio, Ollama y LlamaEdge dependen de llama.cpp, por lo que los modelos GGUF también se usan con frecuencia.

### **¿Qué es GGUF?**

GGUF es un formato binario optimizado para la carga y el guardado rápido de modelos, lo que lo hace altamente eficiente para propósitos de inferencia. GGUF está diseñado para su uso con GGML y otros ejecutores. GGUF fue desarrollado por @ggerganov, quien también es el desarrollador de llama.cpp, un popular framework de inferencia LLM en C/C++. Los modelos inicialmente desarrollados en frameworks como PyTorch pueden ser convertidos al formato GGUF para su uso con esos motores.

### **ONNX vs GGUF**

ONNX es un formato tradicional de aprendizaje automático/profundo, que está bien soportado en diferentes frameworks de IA y tiene buenos escenarios de uso en dispositivos de borde. En cuanto a GGUF, está basado en llama.cpp y se puede decir que es producido en la era de GenAI. Ambos tienen usos similares. Si quieres mejor rendimiento en hardware embebido y capas de aplicación, ONNX puede ser tu elección. Si usas el framework y la tecnología derivada de llama.cpp, entonces GGUF puede ser mejor.

### **Cuantización de Phi-3.5-Instruct usando llama.cpp**

**1. Configuración del Entorno**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. Cuantización**

Usando llama.cpp para convertir Phi-3.5-Instruct a FP16 GGUF


```bash

./convert_hf_to_gguf.py <Tu ubicación de Phi-3.5-Instruct> --outfile phi-3.5-128k-mini_fp16.gguf

```

Cuantizando Phi-3.5 a INT4


```bash

./llama.cpp/llama-quantize <Tu ubicación de phi-3.5-128k-mini_fp16.gguf> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. Pruebas**

Instalar llama-cpp-python


```bash

pip install llama-cpp-python -U

```

***Nota*** 

Si usas Apple Silicon, por favor instala llama-cpp-python de esta manera


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

Pruebas 


```bash

llama.cpp/llama-cli --model <Tu ubicación de phi-3.5-128k-mini_Q4_K_M.gguf> --prompt "<|user|>\n¿Puedes presentar .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **Recursos**

1. Aprende más sobre llama.cpp [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. Aprende más sobre GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

Aviso legal: La traducción fue realizada a partir del original por un modelo de IA y puede no ser perfecta. Por favor, revise el resultado y haga las correcciones necesarias.